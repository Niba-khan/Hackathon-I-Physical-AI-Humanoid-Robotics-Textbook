"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[10],{4715:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var i=t(4848),o=t(8453);const a={},s="Chapter 2: Voice-to-Action (Whisper) - Voice Capture, Transcription, Intent Extraction",r={id:"content/modules/vla/chapter-2",title:"Chapter 2: Voice-to-Action (Whisper) - Voice Capture, Transcription, Intent Extraction",description:"Objectives",source:"@site/docs/content/modules/004-vla/chapter-2.md",sourceDirName:"content/modules/004-vla",slug:"/content/modules/vla/chapter-2",permalink:"/hackathon-textbook/docs/content/modules/vla/chapter-2",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/content/modules/004-vla/chapter-2.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 1: Intro to Vision-Language-Action - VLA Concepts, LLM Role in Humanoid Control",permalink:"/hackathon-textbook/docs/content/modules/vla/chapter-1"},next:{title:"Chapter 3: Cognitive Planning & Capstone - NL \u2192 ROS 2 Action Mapping, Navigation, Obstacle Avoidance, Object ID & Manipulation",permalink:"/hackathon-textbook/docs/content/modules/vla/chapter-3"}},c={},l=[{value:"Objectives",id:"objectives",level:2},{value:"Introduction to Voice Processing for Humanoid Robots",id:"introduction-to-voice-processing-for-humanoid-robots",level:2},{value:"Voice Capture and Preprocessing",id:"voice-capture-and-preprocessing",level:2},{value:"Audio Input Requirements",id:"audio-input-requirements",level:3},{value:"Audio Preprocessing Pipeline",id:"audio-preprocessing-pipeline",level:3},{value:"Whisper Integration for Transcription",id:"whisper-integration-for-transcription",level:2},{value:"Setting up Whisper for Robot Applications",id:"setting-up-whisper-for-robot-applications",level:3},{value:"Whisper Implementation",id:"whisper-implementation",level:3},{value:"Intent Extraction from Transcribed Commands",id:"intent-extraction-from-transcribed-commands",level:2},{value:"Natural Language Processing for Intent Recognition",id:"natural-language-processing-for-intent-recognition",level:3},{value:"Intent Extraction Implementation",id:"intent-extraction-implementation",level:3},{value:"Integration with the VLA Pipeline",id:"integration-with-the-vla-pipeline",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:3},{value:"Voice Command Validation and Error Handling",id:"voice-command-validation-and-error-handling",level:2},{value:"Accuracy Validation",id:"accuracy-validation",level:3},{value:"Hands-on Exercise 2.1: Voice Processing Pipeline",id:"hands-on-exercise-21-voice-processing-pipeline",level:2},{value:"Hands-on Exercise 2.2: Intent Recognition",id:"hands-on-exercise-22-intent-recognition",level:2},{value:"Validation Checklist",id:"validation-checklist",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-2-voice-to-action-whisper---voice-capture-transcription-intent-extraction",children:"Chapter 2: Voice-to-Action (Whisper) - Voice Capture, Transcription, Intent Extraction"}),"\n",(0,i.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand voice processing for humanoid robot control"}),"\n",(0,i.jsx)(e.li,{children:"Implement Whisper-based voice command transcription"}),"\n",(0,i.jsx)(e.li,{children:"Extract intent from transcribed commands"}),"\n",(0,i.jsx)(e.li,{children:"Integrate voice processing into the VLA pipeline"}),"\n",(0,i.jsx)(e.li,{children:"Validate voice command accuracy and robustness"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-voice-processing-for-humanoid-robots",children:"Introduction to Voice Processing for Humanoid Robots"}),"\n",(0,i.jsx)(e.p,{children:"Voice interaction provides the most natural and intuitive interface for human-robot communication. For humanoid robots specifically, voice processing enables:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Hands-free operation for users"}),"\n",(0,i.jsx)(e.li,{children:"Natural communication aligned with human expectations"}),"\n",(0,i.jsx)(e.li,{children:"Operation at a distance without requiring physical interfaces"}),"\n",(0,i.jsx)(e.li,{children:"Multimodal interaction combining voice with gestures and visual cues"}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"In the VLA (Vision-Language-Action) pipeline, voice processing serves as the first stage, converting human speech into text that can be processed by the language understanding system."}),"\n",(0,i.jsx)(e.h2,{id:"voice-capture-and-preprocessing",children:"Voice Capture and Preprocessing"}),"\n",(0,i.jsx)(e.h3,{id:"audio-input-requirements",children:"Audio Input Requirements"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots need to capture high-quality audio in potentially challenging acoustic environments:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Hardware Considerations"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Directional microphone arrays for noise reduction"}),"\n",(0,i.jsx)(e.li,{children:"Multiple microphones for spatial audio processing"}),"\n",(0,i.jsx)(e.li,{children:"Audio preprocessing capabilities for background noise reduction"}),"\n",(0,i.jsx)(e.li,{children:"Integration with the robot's head for natural voice interaction"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Environmental Challenges"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Background noise from mechanical systems"}),"\n",(0,i.jsx)(e.li,{children:"Room acoustics and reverberation"}),"\n",(0,i.jsx)(e.li,{children:"Competing voices or sounds"}),"\n",(0,i.jsx)(e.li,{children:"Distance variations between speaker and robot"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"audio-preprocessing-pipeline",children:"Audio Preprocessing Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom audio_common_msgs.msg import AudioData\nimport numpy as np\nimport pyaudio\nimport webrtcvad\nfrom collections import deque\nimport threading\nimport time\n\nclass VoiceCaptureNode(Node):\n    def __init__(self):\n        super().__init__('voice_capture_node')\n        \n        # Audio configuration\n        self.sample_rate = 16000  # Whisper works best at 16kHz\n        self.chunk_size = 1024\n        self.channels = 1\n        self.format = pyaudio.paInt16  # 16-bit samples\n        \n        # VAD (Voice Activity Detection) for speech detection\n        self.vad = webrtcvad.Vad()\n        self.vad.set_mode(1)  # Aggressiveness level (0-3)\n        \n        # Audio buffer for voice activity detection\n        self.audio_buffer = deque(maxlen=int(self.sample_rate * 2))  # 2 seconds buffer\n        self.is_speaking = False\n        self.speech_segments = []\n        \n        # Publishers for detected speech\n        self.speech_pub = self.create_publisher(\n            AudioData, \n            '/detected_speech', \n            10\n        )\n        \n        # Publishers for transcription requests\n        self.transcription_request_pub = self.create_publisher(\n            String,\n            '/transcription_request',\n            10\n        )\n        \n        # Start audio capture in a separate thread\n        self.audio_thread = threading.Thread(target=self.capture_audio)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n        \n        # Timer for monitoring voice activity\n        self.voice_activity_timer = self.create_timer(0.1, self.check_voice_activity)\n        \n        self.get_logger().info('Voice Capture Node initialized')\n    \n    def capture_audio(self):\n        \"\"\"Capture audio from microphone in a separate thread\"\"\"\n        audio = pyaudio.PyAudio()\n        \n        try:\n            stream = audio.open(\n                format=self.format,\n                channels=self.channels,\n                rate=self.sample_rate,\n                input=True,\n                frames_per_buffer=self.chunk_size\n            )\n            \n            self.get_logger().info('Audio stream started')\n            \n            while rclpy.ok():\n                # Read audio data from the microphone\n                audio_data = stream.read(self.chunk_size, exception_on_overflow=False)\n                \n                # Convert to numpy array for processing\n                audio_array = np.frombuffer(audio_data, dtype=np.int16)\n                \n                # Add to buffer for VAD analysis\n                self.audio_buffer.extend(audio_array)\n                \n                # Check if voice activity is detected\n                voice_active = self.is_voice_active(audio_data)\n                \n                if voice_active and not self.is_speaking:\n                    # Start of speech detected\n                    self.is_speaking = True\n                    self.speech_segments = [audio_data]\n                    self.get_logger().info('Voice activity started')\n                \n                elif not voice_active and self.is_speaking:\n                    # End of speech detected\n                    self.is_speaking = False\n                    self.publish_speech_segment()\n                    self.get_logger().info('Voice activity ended')\n                \n                elif self.is_speaking:\n                    # Continue collecting speech\n                    self.speech_segments.append(audio_data)\n                \n                time.sleep(0.01)  # 10ms sleep\n                \n        except Exception as e:\n            self.get_logger().error(f'Audio capture error: {e}')\n        finally:\n            stream.stop_stream()\n            stream.close()\n            audio.terminate()\n    \n    def is_voice_active(self, audio_chunk):\n        \"\"\"Check if voice activity is present in the audio chunk\"\"\"\n        try:\n            # WebRTCVAD expects 10, 20, or 30ms frames at 8kHz, 16kHz, or 32kHz\n            # For 16kHz and 1024 samples: 1024/16000 = 64ms (too long)\n            # We'll break it down into smaller chunks\n            chunk_duration = 10  # 10ms chunks for VAD\n            samples_per_chunk = int(self.sample_rate * chunk_duration / 1000)\n            \n            # Process the audio in 10ms chunks\n            bytes_per_sample = 2  # 16-bit = 2 bytes\n            total_bytes = len(audio_chunk)\n            \n            for start in range(0, total_bytes, samples_per_chunk * bytes_per_sample):\n                end = min(start + samples_per_chunk * bytes_per_sample, total_bytes)\n                chunk = audio_chunk[start:end]\n                \n                # Pad if necessary\n                if len(chunk) < samples_per_chunk * bytes_per_sample:\n                    chunk += b'\\x00' * ((samples_per_chunk * bytes_per_sample) - len(chunk))\n                \n                # Check VAD\n                if self.vad.is_speech(chunk, self.sample_rate):\n                    return True\n            \n            return False\n        except Exception as e:\n            self.get_logger().error(f'VAD error: {e}')\n            return False\n    \n    def publish_speech_segment(self):\n        \"\"\"Publish collected speech segment for transcription\"\"\"\n        if not self.speech_segments:\n            return\n        \n        # Combine all speech chunks\n        combined_audio = b''.join(self.speech_segments)\n        \n        # Publish to speech detection topic\n        audio_msg = AudioData()\n        audio_msg.data = combined_audio\n        self.speech_pub.publish(audio_msg)\n        \n        # Also publish for transcription request\n        request_msg = String()\n        request_msg.data = combined_audio.hex()  # Send as hex string for processing\n        self.transcription_request_pub.publish(request_msg)\n        \n        self.speech_segments = []  # Clear the segments\n    \n    def check_voice_activity(self):\n        \"\"\"Periodic check for voice activity status\"\"\"\n        # This could be used for UI feedback or other status updates\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCaptureNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"whisper-integration-for-transcription",children:"Whisper Integration for Transcription"}),"\n",(0,i.jsx)(e.h3,{id:"setting-up-whisper-for-robot-applications",children:"Setting up Whisper for Robot Applications"}),"\n",(0,i.jsx)(e.p,{children:"OpenAI's Whisper model provides state-of-the-art automatic speech recognition (ASR) capabilities. For humanoid robot applications, Whisper offers several advantages:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Robustness"}),": Performs well in noisy environments"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-language support"}),": Can handle multiple languages and accents"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-time capability"}),": Can be optimized for near real-time transcription"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Open-source"}),": Available for deployment on robot hardware"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"whisper-implementation",children:"Whisper Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import torch\nimport whisper\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\nimport io\nimport wave\nimport tempfile\nimport numpy as np\nfrom scipy.io import wavfile\n\nclass WhisperTranscriptionNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_transcription_node\')\n        \n        # Load Whisper model (use smaller model for robot deployment)\n        self.get_logger().info(\'Loading Whisper model...\')\n        self.model = whisper.load_model("base.en")  # Use "base" for lower computational needs\n        \n        # Subscriptions for audio input\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            \'/detected_speech\',\n            self.audio_callback,\n            10\n        )\n        \n        # Publishers for transcription results\n        self.transcription_pub = self.create_publisher(\n            String,\n            \'/transcription_result\',\n            10\n        )\n        \n        # Publishers for VLA pipeline\n        self.vla_command_pub = self.create_publisher(\n            String,\n            \'/vla_language_command\',\n            10\n        )\n        \n        self.get_logger().info(\'Whisper Transcription Node initialized\')\n    \n    def audio_callback(self, msg):\n        """Process incoming audio data with Whisper"""\n        try:\n            # Convert audio data from bytes back to waveform\n            audio_bytes = bytes.fromhex(msg.data) if isinstance(msg.data, str) else msg.data\n            \n            # Write audio to temporary WAV file for Whisper\n            with tempfile.NamedTemporaryFile(suffix=\'.wav\', delete=False) as temp_wav:\n                # Create WAV file from raw audio data\n                self.save_audio_to_wav(audio_bytes, temp_wav.name)\n                \n                # Transcribe the audio\n                result = self.model.transcribe(temp_wav.name)\n                \n                # Extract transcription and confidence\n                transcription = result["text"].strip()\n                confidence = self.estimate_confidence(result)\n                \n                self.get_logger().info(f\'Transcribed: "{transcription}" (confidence: {confidence:.2f})\')\n                \n                # Validate transcription quality\n                if self.is_valid_transcription(transcription, confidence):\n                    # Publish transcription result\n                    transcription_msg = String()\n                    transcription_msg.data = transcription\n                    self.transcription_pub.publish(transcription_msg)\n                    \n                    # Also send to VLA pipeline\n                    self.vla_command_pub.publish(transcription_msg)\n                else:\n                    self.get_logger().warn(f\'Low quality transcription rejected: "{transcription}"\')\n        \n        except Exception as e:\n            self.get_logger().error(f\'Whisper transcription error: {e}\')\n    \n    def save_audio_to_wav(self, audio_bytes, filename):\n        """Save raw audio bytes to WAV file for Whisper processing"""\n        # Convert bytes back to numpy array\n        audio_array = np.frombuffer(audio_bytes, dtype=np.int16)\n        \n        # Write to WAV file using scipy\n        wavfile.write(filename, 16000, audio_array)\n    \n    def estimate_confidence(self, result):\n        """Estimate confidence of transcription (simplified approach)"""\n        # In a real implementation, this would use more sophisticated methods\n        # based on token probabilities or other model outputs\n        \n        # For now, use a simplified approach based on:\n        # - Number of tokens in the result\n        # - Average probability if available\n        # - Presence of common speech patterns\n        \n        text = result.get("text", "")\n        if not text:\n            return 0.0\n        \n        # Check for common non-speech patterns\n        non_speech_patterns = ["you", "the", "and", "a", "to", "i", "it", "for", "is", "that", "on", "with", "as", "be", "at"]\n        words = text.lower().split()\n        \n        # A very basic confidence metric\n        # In practice, this should use model\'s internal probabilities\n        valid_word_ratio = sum(1 for word in words if word not in non_speech_patterns) / len(words) if words else 0\n        length_factor = min(1.0, len(text) / 100)  # Normalize by expected length\n        \n        # This is a simplified confidence measure\n        # Real implementation should use Whisper\'s internal probability outputs\n        return min(1.0, (valid_word_ratio + length_factor) / 2)\n    \n    def is_valid_transcription(self, text, confidence):\n        """Validate if transcription meets quality thresholds"""\n        if not text or len(text.strip()) < 2:\n            return False\n        \n        if confidence < 0.5:  # Set confidence threshold\n            return False\n        \n        # Additional validation could check for:\n        # - Profanity filtering\n        # - Command relevance\n        # - Grammar patterns\n        \n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperTranscriptionNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"intent-extraction-from-transcribed-commands",children:"Intent Extraction from Transcribed Commands"}),"\n",(0,i.jsx)(e.h3,{id:"natural-language-processing-for-intent-recognition",children:"Natural Language Processing for Intent Recognition"}),"\n",(0,i.jsx)(e.p,{children:"Once speech is transcribed, the system must extract the user's intent. This involves:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Command Recognition"}),": Identifying the primary action requested"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Entity Extraction"}),": Identifying objects, locations, or other parameters"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Resolution"}),": Understanding references based on visual context"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Ambiguity Resolution"}),": Clarifying unclear references"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"intent-extraction-implementation",children:"Intent Extraction Implementation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import spacy\nimport re\nfrom typing import Dict, List, Tuple\n\nclass IntentExtractionNode(Node):\n    def __init__(self):\n        super().__init__('intent_extraction_node')\n        \n        # Load spaCy model for NLP processing\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            self.get_logger().warn(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n        \n        # Subscriptions\n        self.transcription_sub = self.create_subscription(\n            String,\n            '/transcription_result',\n            self.transcription_callback,\n            10\n        )\n        \n        # Publishers\n        self.intent_pub = self.create_publisher(\n            String,  # In practice, this would be a structured Intent message\n            '/extracted_intent',\n            10\n        )\n        \n        self.vla_task_pub = self.create_publisher(\n            String,\n            '/vla_task_plan',\n            10\n        )\n        \n        # Define common commands and their patterns\n        self.command_patterns = {\n            'navigation': [\n                r'go to (.+)',\n                r'go to the (.+)',\n                r'move to (.+)',\n                r'walk to (.+)',\n                r'go (.+)',\n                r'head to (.+)',\n                r'take me to (.+)'\n            ],\n            'fetch': [\n                r'bring me (.+)',\n                r'get (.+)',\n                r'fetch (.+)',\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'give me (.+)'\n            ],\n            'manipulation': [\n                r'pick (.+)',\n                r'put (.+) (?:on|in) (.+)',\n                r'move (.+) (?:to|on|in) (.+)',\n                r'place (.+) (?:on|in) (.+)',\n                r'open (.+)',\n                r'close (.+)'\n            ],\n            'interaction': [\n                r'wave to (.+)',\n                r'say hello to (.+)',\n                r'greet (.+)',\n                r'talk to (.+)',\n                r'follow (.+)'\n            ]\n        }\n        \n        self.get_logger().info('Intent Extraction Node initialized')\n    \n    def transcription_callback(self, msg):\n        \"\"\"Process transcribed text and extract intent\"\"\"\n        text = msg.data\n        \n        self.get_logger().info(f'Processing transcription: \"{text}\"')\n        \n        # Extract intent using pattern matching\n        intent = self.extract_intent_nlp(text)\n        \n        if intent:\n            # Validate and refine the intent\n            validated_intent = self.validate_intent(intent)\n            \n            if validated_intent:\n                # Publish the extracted intent\n                intent_msg = String()\n                intent_msg.data = json.dumps(validated_intent)\n                self.intent_pub.publish(intent_msg)\n                \n                # Also send to VLA pipeline\n                self.vla_task_pub.publish(intent_msg)\n                \n                self.get_logger().info(f'Intent extracted: {validated_intent}')\n            else:\n                self.get_logger().warn('Intent validation failed')\n        else:\n            self.get_logger().warn(f'No intent recognized in: \"{text}\"')\n    \n    def extract_intent_nlp(self, text: str) -> Dict:\n        \"\"\"Extract intent using NLP techniques and pattern matching\"\"\"\n        if not self.nlp:\n            # Fallback to simple pattern matching if spaCy is not available\n            return self.extract_intent_patterns(text)\n        \n        # Process text with spaCy\n        doc = self.nlp(text.lower())\n        \n        # Extract tokens and their dependencies\n        tokens = [(token.text, token.pos_, token.dep_) for token in doc]\n        \n        # Identify command type using pattern matching on tokens\n        command_type = self.identify_command_type(text)\n        \n        if command_type:\n            # Extract entities based on the command type\n            entities = self.extract_entities(doc, command_type)\n            \n            return {\n                'command_type': command_type,\n                'entities': entities,\n                'raw_text': text,\n                'confidence': 0.8  # Placeholder confidence\n            }\n        \n        return None\n    \n    def extract_intent_patterns(self, text: str) -> Dict:\n        \"\"\"Simple pattern matching for intent extraction (fallback)\"\"\"\n        text_lower = text.lower()\n        \n        for command_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text_lower)\n                if match:\n                    groups = match.groups()\n                    \n                    # Handle different numbers of groups for different patterns\n                    if len(groups) == 1:\n                        entities = {'target': groups[0]}\n                    elif len(groups) == 2:\n                        entities = {'target': groups[0], 'destination': groups[1]}\n                    else:\n                        entities = {f'arg{i}': arg for i, arg in enumerate(groups)}\n                    \n                    return {\n                        'command_type': command_type,\n                        'entities': entities,\n                        'raw_text': text,\n                        'confidence': 0.7  # Lower confidence for pattern matching\n                    }\n        \n        return None\n    \n    def identify_command_type(self, text: str) -> str:\n        \"\"\"Identify the type of command using pattern matching\"\"\"\n        return self.extract_intent_patterns(text)['command_type'] if self.extract_intent_patterns(text) else None\n    \n    def extract_entities(self, doc, command_type: str) -> Dict:\n        \"\"\"Extract named entities and objects based on command type\"\"\"\n        entities = {}\n        \n        # Extract noun phrases and named entities\n        for ent in doc.ents:\n            entities[ent.label_.lower()] = ent.text\n        \n        # Extract specific objects based on command type\n        if command_type == 'fetch':\n            # Look for direct objects (dobj) in the sentence\n            for token in doc:\n                if token.dep_ == 'dobj':  # Direct object\n                    # Get the noun phrase for the object\n                    entities['object'] = self.get_noun_phrase(token)\n        \n        elif command_type == 'navigation':\n            # Look for locations (pobj - object of preposition)\n            for token in doc:\n                if token.dep_ == 'pobj':\n                    # Check if this is a location\n                    if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n                        entities['location'] = token.text\n            \n            # Also look for adverbial phrases that might indicate location\n            for token in doc:\n                if token.dep_ == 'advmod' and token.tag_ in ['NN', 'NNS']:\n                    entities['destination'] = token.text\n        \n        elif command_type == 'manipulation':\n            # Extract direct objects and prepositional objects\n            for token in doc:\n                if token.dep_ == 'dobj':\n                    entities['object'] = self.get_noun_phrase(token)\n                elif token.dep_ == 'pobj':\n                    entities['target'] = self.get_noun_phrase(token)\n        \n        return entities\n    \n    def get_noun_phrase(self, token) -> str:\n        \"\"\"Get the complete noun phrase starting from the token\"\"\"\n        # In a real implementation, this would collect the entire noun phrase\n        # including adjectives, determiners, etc.\n        return token.text\n    \n    def validate_intent(self, intent: Dict) -> Dict:\n        \"\"\"Validate and refine extracted intent\"\"\"\n        if not intent:\n            return None\n        \n        # Validate confidence threshold\n        if intent.get('confidence', 0) < 0.5:\n            return None\n        \n        # Validate entity completeness based on command type\n        required_entities = self.get_required_entities(intent['command_type'])\n        missing_entities = []\n        \n        for entity_type in required_entities:\n            if entity_type not in intent['entities']:\n                missing_entities.append(entity_type)\n        \n        if missing_entities and intent['command_type'] != 'interaction':\n            # For critical commands, require all entities\n            self.get_logger().warn(f'Missing required entities: {missing_entities}')\n            return None\n        \n        # Refine entities using knowledge base\n        intent['entities'] = self.refine_entities(intent['entities'])\n        \n        return intent\n    \n    def get_required_entities(self, command_type: str) -> List[str]:\n        \"\"\"Get required entities for different command types\"\"\"\n        required = {\n            'navigation': ['location'],\n            'fetch': ['object'],\n            'manipulation': ['object'],\n            'interaction': []  # Interaction might not always require objects\n        }\n        \n        return required.get(command_type, [])\n    \n    def refine_entities(self, entities: Dict) -> Dict:\n        \"\"\"Refine entities using knowledge base and context\"\"\"\n        # In a real implementation, this would use:\n        # - Visual context to resolve ambiguous references\n        # - Knowledge base to resolve common object names\n        # - Spatial context to resolve location references\n        \n        # Example: normalize object names\n        if 'object' in entities:\n            entities['object'] = self.normalize_object_name(entities['object'])\n        \n        return entities\n    \n    def normalize_object_name(self, name: str) -> str:\n        \"\"\"Normalize object names to standard forms\"\"\"\n        # Map common variations to standard names\n        name_map = {\n            'coffee cup': 'coffee_mug',\n            'water cup': 'glass',\n            'water glass': 'glass',\n            'book': 'book',\n            'the book': 'book',\n            'it': 'object'  # Context-dependent, would need visual resolution\n        }\n        \n        return name_map.get(name, name)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IntentExtractionNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-the-vla-pipeline",children:"Integration with the VLA Pipeline"}),"\n",(0,i.jsx)(e.h3,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"The complete voice-to-action pipeline integrates all components:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class VoiceToActionNode(Node):\n    def __init__(self):\n        super().__init__('voice_to_action_node')\n        \n        # Initialize all pipeline components\n        self.voice_capture = VoiceCaptureNode()\n        self.whisper_transcription = WhisperTranscriptionNode()\n        self.intent_extraction = IntentExtractionNode()\n        \n        # Subscriptions\n        self.intent_sub = self.create_subscription(\n            String,\n            '/extracted_intent',\n            self.intent_callback,\n            10\n        )\n        \n        # Publishers for downstream VLA components\n        self.task_plan_pub = self.create_publisher(\n            String,\n            '/vla_task_plan',\n            10\n        )\n        \n        # Publishers for user feedback\n        self.feedback_pub = self.create_publisher(\n            String,\n            '/voice_feedback',\n            10\n        )\n        \n        self.get_logger().info('Voice-to-Action Pipeline initialized')\n    \n    def intent_callback(self, msg):\n        \"\"\"Process extracted intent and generate task plan\"\"\"\n        try:\n            intent_data = json.loads(msg.data)\n            \n            # Generate task plan based on intent\n            task_plan = self.generate_task_plan(intent_data)\n            \n            if task_plan:\n                # Publish task plan to VLA pipeline\n                plan_msg = String()\n                plan_msg.data = json.dumps(task_plan)\n                self.task_plan_pub.publish(plan_msg)\n                \n                # Provide feedback to user\n                feedback_msg = String()\n                feedback_msg.data = f\"Executing: {intent_data.get('command_type', 'unknown')} command\"\n                self.feedback_pub.publish(feedback_msg)\n                \n                self.get_logger().info(f'Task plan generated: {task_plan}')\n            else:\n                self.get_logger().warn('Failed to generate task plan from intent')\n                \n        except Exception as e:\n            self.get_logger().error(f'Error processing intent: {e}')\n    \n    def generate_task_plan(self, intent_data):\n        \"\"\"Generate executable task plan from extracted intent\"\"\"\n        command_type = intent_data.get('command_type')\n        entities = intent_data.get('entities', {})\n        \n        if command_type == 'navigation':\n            return self.generate_navigation_plan(entities)\n        elif command_type == 'fetch':\n            return self.generate_fetch_plan(entities)\n        elif command_type == 'manipulation':\n            return self.generate_manipulation_plan(entities)\n        elif command_type == 'interaction':\n            return self.generate_interaction_plan(entities)\n        else:\n            return None\n    \n    def generate_navigation_plan(self, entities):\n        \"\"\"Generate navigation task plan\"\"\"\n        location = entities.get('location') or entities.get('destination')\n        \n        if not location:\n            return None\n        \n        return {\n            'action': 'navigate',\n            'target_location': location,\n            'parameters': {\n                'speed': 'normal',\n                'avoidance_mode': 'cautious'\n            }\n        }\n    \n    def generate_fetch_plan(self, entities):\n        \"\"\"Generate fetch task plan\"\"\"\n        object_name = entities.get('object')\n        \n        if not object_name:\n            return None\n        \n        return {\n            'action': 'fetch',\n            'target_object': object_name,\n            'parameters': {\n                'approach_method': 'frontal',\n                'grip_type': 'precision_pinch',\n                'delivery_method': 'hand_off'\n            }\n        }\n    \n    def generate_manipulation_plan(self, entities):\n        \"\"\"Generate manipulation task plan\"\"\"\n        object_name = entities.get('object') or entities.get('target')\n        target_location = entities.get('destination') or entities.get('target')\n        \n        if not object_name or not target_location:\n            return None\n        \n        return {\n            'action': 'manipulation',\n            'target_object': object_name,\n            'target_location': target_location,\n            'parameters': {\n                'motion': 'place',\n                'orientation': 'upright'\n            }\n        }\n    \n    def generate_interaction_plan(self, entities):\n        \"\"\"Generate interaction task plan\"\"\"\n        target_entity = entities.get('target')\n        \n        if target_entity:\n            if 'wave' in entities.get('raw_text', '').lower():\n                action = 'wave'\n            elif any(word in ['hello', 'hi', 'greet'] for word in entities.get('raw_text', '').lower().split()):\n                action = 'greet'\n            else:\n                action = 'acknowledge'\n        else:\n            action = 'acknowledge'\n        \n        return {\n            'action': action,\n            'target': target_entity,\n            'parameters': {\n                'style': 'polite',\n                'distance': 'arm_length'\n            }\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceToActionNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"voice-command-validation-and-error-handling",children:"Voice Command Validation and Error Handling"}),"\n",(0,i.jsx)(e.h3,{id:"accuracy-validation",children:"Accuracy Validation"}),"\n",(0,i.jsx)(e.p,{children:"Voice-to-action systems must validate command accuracy to prevent incorrect robot behavior:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class VoiceValidationNode(Node):\n    def __init__(self):\n        super().__init__('voice_validation_node')\n        \n        # Subscriptions\n        self.intent_sub = self.create_subscription(\n            String,\n            '/extracted_intent',\n            self.intent_callback,\n            10\n        )\n        \n        self.transcription_sub = self.create_subscription(\n            String,\n            '/transcription_result',\n            self.transcription_callback,\n            10\n        )\n        \n        # Publishers\n        self.validated_intent_pub = self.create_publisher(\n            String,\n            '/validated_intent',\n            10\n        )\n        \n        self.correction_request_pub = self.create_publisher(\n            String,\n            '/correction_request',\n            10\n        )\n        \n        # Internal state\n        self.transcription_history = []\n        self.validation_threshold = 0.7\n        self.confidence_history = deque(maxlen=5)\n        \n    def intent_callback(self, msg):\n        \"\"\"Validate intent before passing it to the VLA pipeline\"\"\"\n        try:\n            intent_data = json.loads(msg.data)\n            \n            # Perform validation checks\n            is_valid = self.validate_intent_completeness(intent_data)\n            confidence = intent_data.get('confidence', 0)\n            \n            # Add confidence to history for trend analysis\n            self.confidence_history.append(confidence)\n            \n            # Check for consistent low confidence\n            avg_confidence = sum(self.confidence_history) / len(self.confidence_history) if self.confidence_history else 0\n            \n            if is_valid and confidence >= self.validation_threshold and avg_confidence > 0.6:\n                # Publish validated intent\n                self.validated_intent_pub.publish(msg)\n                self.get_logger().info(f'Intent validated with confidence {confidence:.2f}')\n            else:\n                # Request clarification or reissuing\n                self.request_clarification(intent_data, confidence < self.validation_threshold)\n                \n        except Exception as e:\n            self.get_logger().error(f'Intent validation error: {e}')\n    \n    def validate_intent_completeness(self, intent_data):\n        \"\"\"Validate that the intent has sufficient information for execution\"\"\"\n        command_type = intent_data.get('command_type')\n        entities = intent_data.get('entities', {})\n        \n        # Check for required entities based on command type\n        required_entities = {\n            'navigation': ['location'],\n            'fetch': ['object'],\n            'manipulation': ['object', 'target'],  # Both needed for manipulation\n            'interaction': ['target']  # For greeting/waving\n        }\n        \n        if command_type in required_entities:\n            required = required_entities[command_type]\n            for req in required:\n                if req not in entities:\n                    self.get_logger().warn(f'Missing required entity \"{req}\" for command type \"{command_type}\"')\n                    return False\n        \n        return True\n    \n    def request_clarification(self, intent_data, low_confidence):\n        \"\"\"Request clarification from the user\"\"\"\n        command_type = intent_data.get('command_type')\n        \n        if low_confidence:\n            # Ask user to repeat the command\n            request_msg = String()\n            request_msg.data = \"I didn't catch that clearly. Could you please repeat your command?\"\n            self.correction_request_pub.publish(request_msg)\n        else:\n            # Ask for missing information\n            missing_info = self.get_missing_info(intent_data)\n            if missing_info:\n                request_msg = String()\n                request_msg.data = f\"Could you specify {missing_info}?\"\n                self.correction_request_pub.publish(request_msg)\n    \n    def get_missing_info(self, intent_data):\n        \"\"\"Determine what information is missing from the intent\"\"\"\n        command_type = intent_data.get('command_type')\n        entities = intent_data.get('entities', {})\n        \n        if command_type == 'navigation':\n            if 'location' not in entities:\n                return \"the location where you'd like me to go\"\n        elif command_type == 'fetch':\n            if 'object' not in entities:\n                return \"which object you'd like me to bring\"\n        elif command_type == 'manipulation':\n            missing = []\n            if 'object' not in entities:\n                missing.append(\"which object\")\n            if 'target' not in entities:\n                missing.append(\"where you'd like to place it\")\n            return \" and \".join(missing) + \" for the manipulation task\" if missing else None\n        \n        return None\n    \n    def transcription_callback(self, msg):\n        \"\"\"Track transcription accuracy over time\"\"\"\n        self.transcription_history.append({\n            'text': msg.data,\n            'timestamp': self.get_clock().now().nanoseconds\n        })\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceValidationNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-21-voice-processing-pipeline",children:"Hands-on Exercise 2.1: Voice Processing Pipeline"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Set up the voice capture system with appropriate audio hardware"}),"\n",(0,i.jsx)(e.li,{children:"Integrate Whisper for real-time transcription"}),"\n",(0,i.jsx)(e.li,{children:"Test the system with various voice commands"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate the transcription accuracy under different acoustic conditions"}),"\n",(0,i.jsx)(e.li,{children:"Implement basic audio preprocessing for noise reduction"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-22-intent-recognition",children:"Hands-on Exercise 2.2: Intent Recognition"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Create a dataset of voice commands specific to your robot's capabilities"}),"\n",(0,i.jsx)(e.li,{children:"Train or configure the intent recognition system for your specific commands"}),"\n",(0,i.jsx)(e.li,{children:"Test the system with natural language commands"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate the accuracy of intent extraction"}),"\n",(0,i.jsx)(e.li,{children:"Implement error handling and clarification requests"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"validation-checklist",children:"Validation Checklist"}),"\n",(0,i.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand the audio capture requirements for humanoid robots"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can implement Whisper-based transcription for voice commands"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can extract intent from transcribed commands using NLP techniques"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have integrated voice processing into the VLA pipeline"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have implemented validation and error handling for voice commands"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have tested the system with various voice commands and acoustic conditions"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand how to improve voice command accuracy and robustness"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have evaluated the overall performance of the voice-to-action pipeline"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter covered the voice processing component of the VLA pipeline, focusing on capturing, transcribing, and understanding voice commands for humanoid robot control. We explored audio capture and preprocessing techniques, Whisper integration for transcription, and NLP-based intent extraction from transcribed commands."}),"\n",(0,i.jsx)(e.p,{children:"The voice-to-action pipeline transforms human speech into structured commands that can be processed by the VLA system, enabling natural and intuitive interaction with humanoid robots. Proper validation and error handling ensure that voice commands are accurately interpreted and safely executed."}),"\n",(0,i.jsx)(e.p,{children:"In the next chapter, we'll explore the complete VLA pipeline execution, including cognitive planning, ROS 2 action mapping, and the capstone implementation of the full voice-to-action autonomous task."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);