"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[342],{1412:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>r,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var i=a(4848),t=a(8453);const o={},s="Chapter 3: Navigation & Path Planning - Isaac ROS VSLAM, Nav2 Integration, Humanoid Path Execution",l={id:"content/modules/ai-robot-brain/chapter-3",title:"Chapter 3: Navigation & Path Planning - Isaac ROS VSLAM, Nav2 Integration, Humanoid Path Execution",description:"Objectives",source:"@site/docs/content/modules/003-ai-robot-brain/chapter-3.md",sourceDirName:"content/modules/003-ai-robot-brain",slug:"/content/modules/ai-robot-brain/chapter-3",permalink:"/docs/content/modules/ai-robot-brain/chapter-3",draft:!1,unlisted:!1,editUrl:"https://github.com/Niba-khan/Hackathon-I-Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/content/modules/003-ai-robot-brain/chapter-3.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 2: Perception & Synthetic Data - Photorealistic Scenes, Sensor Simulation, Dataset Generation",permalink:"/docs/content/modules/ai-robot-brain/chapter-2"},next:{title:"Module 3 Validation Checklist",permalink:"/docs/content/modules/ai-robot-brain/checklists/validation-checklist"}},r={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Introduction to VSLAM for Humanoid Robotics",id:"introduction-to-vslam-for-humanoid-robotics",level:2},{value:"Understanding VSLAM Fundamentals",id:"understanding-vslam-fundamentals",level:2},{value:"Core Concepts",id:"core-concepts",level:3},{value:"Challenges for Humanoid Robots",id:"challenges-for-humanoid-robots",level:3},{value:"Isaac ROS Visual SLAM Implementation",id:"isaac-ros-visual-slam-implementation",level:2},{value:"Isaac ROS Visual SLAM Pipeline",id:"isaac-ros-visual-slam-pipeline",level:3},{value:"Setting Up Isaac ROS Visual SLAM",id:"setting-up-isaac-ros-visual-slam",level:3},{value:"Isaac ROS Visual SLAM Parameters",id:"isaac-ros-visual-slam-parameters",level:3},{value:"Integration with Nav2 Navigation Stack",id:"integration-with-nav2-navigation-stack",level:2},{value:"Nav2 Overview",id:"nav2-overview",level:3},{value:"Nav2 Configuration for Humanoid Robots",id:"nav2-configuration-for-humanoid-robots",level:3},{value:"Launch File for VSLAM + Nav2 Integration",id:"launch-file-for-vslam--nav2-integration",level:3},{value:"Humanoid-Specific Navigation Considerations",id:"humanoid-specific-navigation-considerations",level:2},{value:"Bipedal Locomotion Challenges",id:"bipedal-locomotion-challenges",level:3},{value:"Humanoid Navigation Pipeline",id:"humanoid-navigation-pipeline",level:3},{value:"Path Execution and Safety",id:"path-execution-and-safety",level:2},{value:"Humanoid Path Execution Challenges",id:"humanoid-path-execution-challenges",level:3},{value:"Safety Mechanisms",id:"safety-mechanisms",level:3},{value:"Hands-on Exercise 3.1: Implement VSLAM + Nav2 Integration",id:"hands-on-exercise-31-implement-vslam--nav2-integration",level:2},{value:"Hands-on Exercise 3.2: Humanoid Navigation with Safety",id:"hands-on-exercise-32-humanoid-navigation-with-safety",level:2},{value:"Validation Checklist",id:"validation-checklist",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-3-navigation--path-planning---isaac-ros-vslam-nav2-integration-humanoid-path-execution",children:"Chapter 3: Navigation & Path Planning - Isaac ROS VSLAM, Nav2 Integration, Humanoid Path Execution"}),"\n",(0,i.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand Visual SLAM (VSLAM) concepts and implementation with Isaac ROS"}),"\n",(0,i.jsx)(e.li,{children:"Integrate Isaac ROS VSLAM with the ROS 2 navigation stack"}),"\n",(0,i.jsx)(e.li,{children:"Configure Nav2 for bipedal humanoid navigation challenges"}),"\n",(0,i.jsx)(e.li,{children:"Execute complete navigation tasks in simulation"}),"\n",(0,i.jsx)(e.li,{children:"Validate navigation performance and safety"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-vslam-for-humanoid-robotics",children:"Introduction to VSLAM for Humanoid Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is critical for humanoid robots to navigate and understand their environment using only visual sensors. Unlike wheeled robots that can rely on wheel odometry, humanoid robots must use visual and inertial sensors for localization and mapping due to their complex locomotion patterns."}),"\n",(0,i.jsx)(e.p,{children:"Isaac ROS provides GPU-accelerated VSLAM capabilities that are particularly beneficial for humanoid robots due to their computational demands and real-time requirements."}),"\n",(0,i.jsx)(e.h2,{id:"understanding-vslam-fundamentals",children:"Understanding VSLAM Fundamentals"}),"\n",(0,i.jsx)(e.h3,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"})," solves the problem of:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Building a map of an unknown environment"}),"\n",(0,i.jsx)(e.li,{children:"Simultaneously localizing the robot within that map"}),"\n",(0,i.jsx)(e.li,{children:"Using only sensor data (in VSLAM, primarily visual data)"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Visual SLAM"})," specifically uses:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Camera images as primary sensor input"}),"\n",(0,i.jsx)(e.li,{children:"Feature detection and tracking (SIFT, ORB, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Geometric constraints from camera motion"}),"\n",(0,i.jsx)(e.li,{children:"Potential depth information from stereo or structured light"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"challenges-for-humanoid-robots",children:"Challenges for Humanoid Robots"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots present unique challenges for VSLAM:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Dynamic Motion"}),": Unlike wheeled robots with predictable motion models, humanoid robots have complex, often irregular movement patterns that make motion estimation difficult."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Sensor Position"}),": The camera on a humanoid robot is typically at head height, providing a different perspective than ground-based robots."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Computational Constraints"}),": Humanoid robots often have limited computational resources while requiring real-time processing."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Occlusions and Motion Blur"}),": Humanoid locomotion can cause motion blur and self-occlusion of the camera view."]}),"\n",(0,i.jsx)(e.h2,{id:"isaac-ros-visual-slam-implementation",children:"Isaac ROS Visual SLAM Implementation"}),"\n",(0,i.jsx)(e.h3,{id:"isaac-ros-visual-slam-pipeline",children:"Isaac ROS Visual SLAM Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"The Isaac ROS Visual SLAM system typically consists of:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Image Preprocessing"}),": Camera calibration, rectification, and distortion correction"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feature Detection"}),": GPU-accelerated detection of visual features"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feature Tracking"}),": Tracking features across frames to estimate motion"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pose Estimation"}),": Estimating robot pose relative to the map"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Mapping"}),": Building and maintaining a consistent map"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Loop Closure"}),": Identifying when the robot revisits previous locations"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"setting-up-isaac-ros-visual-slam",children:"Setting Up Isaac ROS Visual SLAM"}),"\n",(0,i.jsx)(e.p,{children:"Here's how to set up Isaac ROS Visual SLAM in a ROS 2 launch file:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-xml",children:'\x3c!-- visual_slam.launch.xml --\x3e\n<launch>\n  \x3c!-- Isaac ROS Visual SLAM node --\x3e\n  <node pkg="isaac_ros_visual_slam" \n        exec="isaac_ros_visual_slam" \n        name="visual_slam" \n        namespace="robot1"\n        output="screen">\n    \n    \x3c!-- Input topics --\x3e\n    <param name="enable_imu_preintegration" value="True"/>\n    <param name="enable_slam_visualization" value="True"/>\n    <param name="enable_landmarks_visibility" value="True"/>\n    <param name="enable_observations_visibility" value="True"/>\n    \n    \x3c!-- Image topic --\x3e\n    <remap from="image" to="/camera/rgb/image_rect_color"/>\n    <remap from="camera_info" to="/camera/rgb/camera_info"/>\n    <remap from="imu" to="/imu/data"/>\n    \n    \x3c!-- Output topics --\x3e\n    <remap from="visual_slam/visual_odometry" to="/robot1/visual_odometry"/>\n    <remap from="visual_slam/tracking/feature_tracks" to="/robot1/feature_tracks"/>\n    <remap from="visual_slam/tracking/gyroscope_timestamp_matched" to="/robot1/gyroscope_matched"/>\n    \n  </node>\n</launch>\n'})}),"\n",(0,i.jsx)(e.h3,{id:"isaac-ros-visual-slam-parameters",children:"Isaac ROS Visual SLAM Parameters"}),"\n",(0,i.jsx)(e.p,{children:"Key parameters for humanoid robot VSLAM:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example configuration for humanoid robot VSLAM\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Bool\n\nclass HumanoidVSLAMNode(Node):\n    def __init__(self):\n        super().__init__('humanoid_vsalm_node')\n        \n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, \n            '/camera/rgb/image_rect_color', \n            self.image_callback, \n            10\n        )\n        \n        self.imu_sub = self.create_subscription(\n            Imu, \n            '/imu/data', \n            self.imu_callback, \n            10\n        )\n        \n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \n            '/camera/rgb/camera_info', \n            self.camera_info_callback, \n            10\n        )\n        \n        # Publishers for VSLAM output\n        self.odom_pub = self.create_publisher(\n            Odometry, \n            '/robot1/visual_odometry', \n            10\n        )\n        \n        self.map_pub = self.create_publisher(\n            PoseStamped, \n            '/robot1/map_pose', \n            10\n        )\n        \n        # Configuration parameters specific to humanoid robots\n        self.declare_parameters(\n            namespace='',\n            parameters=[\n                ('max_features', 2000),      # More features for complex scenes\n                ('min_features', 100),       # Minimum for tracking\n                ('max_pose_covariance', 0.1), # Position covariance threshold\n                ('min_translation', 0.05),   # Minimum translation for keyframe\n                ('min_rotation', 0.087),     # Minimum rotation (5 degrees) for keyframe\n                ('enable_loop_closure', True), # Critical for humanoid long-term navigation\n                ('enable_mapping', True),\n                ('use_imu', True),           # IMU integration important for humanoid stability\n            ]\n        )\n        \n        self.get_logger().info('Humanoid VSLAM node initialized')\n        \n    def image_callback(self, msg):\n        # Process image for visual SLAM\n        # In practice, this would interface with Isaac ROS Visual SLAM\n        self.get_logger().info(f'Received image: {msg.width}x{msg.height}')\n        \n    def imu_callback(self, msg):\n        # Use IMU data to improve pose estimation\n        # Humanoid robots benefit from IMU integration for balance-aware SLAM\n        self.get_logger().info('Received IMU data')\n        \n    def camera_info_callback(self, msg):\n        # Camera calibration data\n        self.get_logger().info(f'Camera calibration: {msg.K}')\n"})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-nav2-navigation-stack",children:"Integration with Nav2 Navigation Stack"}),"\n",(0,i.jsx)(e.h3,{id:"nav2-overview",children:"Nav2 Overview"}),"\n",(0,i.jsx)(e.p,{children:"The Navigation2 (Nav2) stack is the ROS 2 navigation framework that provides:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Path planning and execution"}),"\n",(0,i.jsx)(e.li,{children:"Costmap management"}),"\n",(0,i.jsx)(e.li,{children:"Behavior trees for complex navigation tasks"}),"\n",(0,i.jsx)(e.li,{children:"Recovery behaviors for challenging situations"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"nav2-configuration-for-humanoid-robots",children:"Nav2 Configuration for Humanoid Robots"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots have different navigation requirements than wheeled robots:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-yaml",children:'# nav2_params_humanoid.yaml\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.5\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_costmap_2d::VoxelLayer"\n    save_pose_rate: 0.5\n    set_initial_pose: false\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.1\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n    scan_topic: /scan\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: map\n    robot_base_frame: base_link\n    odom_topic: /odom\n    default_bt_xml_filename: "navigate_w_replanning_and_recovery.xml"\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_single_trigger_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.001\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n    \n    # Humanoid-specific velocity limits\n    FollowPath:\n      plugin: "nav2_mppi_controller::MppiController"\n      time_steps: 26\n      control_freq: 40.0\n      horizon: 1.3\n      Q: [1.0, 1.0, 0.05]\n      R: [1.0, 1.0, 0.1]\n      motion_model: "DiffDrive"\n      # Humanoid-specific constraints\n      max_speed: 0.5        # Slower than wheeled robots for stability\n      min_speed: -0.2       # Allow limited backward movement\n      max_accel: 0.5        # Conservative acceleration for balance\n      max_decel: -1.0       # Can stop more quickly\n      max_rot_speed: 0.7    # Slower turns for balance\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 10.0\n      publish_frequency: 10.0\n      global_frame: odom\n      robot_base_frame: base_link\n      use_sim_time: True\n      rolling_window: true\n      width: 6\n      height: 6\n      resolution: 0.05\n      # Humanoid-specific settings\n      footprint: "[ [0.3, 0.15], [0.3, -0.15], [-0.1, -0.15], [-0.1, 0.15] ]"\n      plugin_names: ["obstacles_layer", "inflation_layer"]\n      plugin_types: ["nav2_costmap_2d::ObstacleLayer", "nav2_costmap_2d::InflationLayer"]\n      obstacles_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        enabled: True\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.4  # Humanoid needs more clearance\n\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 1.0\n      publish_frequency: 1.0\n      global_frame: map\n      robot_base_frame: base_link\n      use_sim_time: True\n      robot_radius: 0.3  # Humanoid footprint\n      resolution: 0.05\n      plugin_names: ["obstacles_layer", "inflation_layer"]\n      plugin_types: ["nav2_costmap_2d::ObstacleLayer", "nav2_costmap_2d::InflationLayer"]\n      obstacles_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        enabled: True\n        cost_scaling_factor: 2.0\n        inflation_radius: 0.6  # Humanoid needs more clearance\n\nplanner_server:\n  ros__parameters:\n    expected_planner_frequency: 5.0\n    planner_plugins: ["GridBased"]\n    GridBased:\n      plugin: "nav2_navfn_planner::NavfnPlanner"\n      tolerance: 0.5    # Larger tolerance for humanoid navigation\n      use_astar: false\n      allow_unknown: true\n'})}),"\n",(0,i.jsx)(e.h3,{id:"launch-file-for-vslam--nav2-integration",children:"Launch File for VSLAM + Nav2 Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-xml",children:'\x3c!-- humanoid_navigation.launch.xml --\x3e\n<launch>\n  \x3c!-- Visual SLAM node --\x3e\n  <include file="$(find-pkg-share isaac_ros_visual_slam)/launch/visual_slam.launch.xml"/>\n  \n  \x3c!-- Nav2 Stack --\x3e\n  <include file="$(find-pkg-share nav2_bringup)/launch/navigation_launch.py">\n    <arg name="use_sim_time" value="True"/>\n    <arg name="params_file" value="$(find-pkg-share your_package)/config/nav2_params_humanoid.yaml"/>\n  </include>\n  \n  \x3c!-- TF transformations for humanoid robot --\x3e\n  <node pkg="robot_state_publisher" \n        exec="robot_state_publisher" \n        name="robot_state_publisher">\n    <param name="robot_description" value="$(var robot_description)"/>\n  </node>\n  \n  \x3c!-- Localization (AMCL) --\x3e\n  <node pkg="nav2_amcl" \n        exec="amcl" \n        name="amcl">\n    <param name="use_sim_time" value="True"/>\n  </node>\n</launch>\n'})}),"\n",(0,i.jsx)(e.h2,{id:"humanoid-specific-navigation-considerations",children:"Humanoid-Specific Navigation Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"bipedal-locomotion-challenges",children:"Bipedal Locomotion Challenges"}),"\n",(0,i.jsx)(e.p,{children:"Navigating with bipedal locomotion presents unique challenges:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Footstep Planning"}),": Unlike wheeled robots, humanoid robots must plan where to place each foot, considering:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Reachability of foot placements"}),"\n",(0,i.jsx)(e.li,{children:"Stability of the center of mass"}),"\n",(0,i.jsx)(e.li,{children:"Obstacle avoidance at ground level"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Balance Maintenance"}),": The navigation system must consider:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Center of mass trajectory"}),"\n",(0,i.jsx)(e.li,{children:"Zero moment point (ZMP) constraints"}),"\n",(0,i.jsx)(e.li,{children:"Swing foot trajectories"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Dynamic Stability"}),": Humanoid robots are dynamically stable, meaning:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"They must maintain motion to stay upright"}),"\n",(0,i.jsx)(e.li,{children:"Stopping abruptly can be challenging"}),"\n",(0,i.jsx)(e.li,{children:"Turning requires careful balance management"}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"humanoid-navigation-pipeline",children:"Humanoid Navigation Pipeline"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Humanoid Navigation Pipeline\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Path, Odometry\nfrom sensor_msgs.msg import LaserScan, Image\nfrom std_msgs.msg import Bool\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\nimport numpy as np\n\nclass HumanoidNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_navigation_node\')\n        \n        # Navigation state\n        self.current_pose = None\n        self.goal_pose = None\n        self.navigation_active = False\n        self.balance_state = "STABLE"  # STABLE, UNSTABLE, RECOVERING\n        \n        # Subscriptions\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/robot1/visual_odometry\', self.odom_callback, 10\n        )\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        \n        self.imu_sub = self.create_subscription(\n            sensor_msgs.msg.Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        \n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'/global_plan\', 10)\n        \n        # Navigation action client\n        self.nav_to_pose_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        \n        # TF listeners\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n        \n        # Timers\n        self.nav_timer = self.create_timer(0.05, self.navigation_step)  # 20Hz navigation control\n        \n        self.get_logger().info(\'Humanoid Navigation Node initialized\')\n    \n    def odom_callback(self, msg):\n        """Update current pose from VSLAM odometry"""\n        self.current_pose = msg.pose.pose\n        # Update navigation state based on VSLAM pose\n        self.update_navigation_state()\n    \n    def scan_callback(self, msg):\n        """Process laser scan for obstacle detection"""\n        # Check for obstacles in planned path\n        if self.navigation_active:\n            self.check_path_clearance(msg)\n    \n    def imu_callback(self, msg):\n        """Monitor IMU for balance state"""\n        # Calculate balance metrics from IMU data\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n        \n        # Simple balance check - in reality this would be more complex\n        balance_ok = abs(orientation.z) < 0.3  # Tilt threshold\n        \n        if not balance_ok:\n            self.balance_state = "UNSTABLE"\n            self.safety_stop()\n        else:\n            self.balance_state = "STABLE"\n    \n    def navigate_to_pose(self, goal_pose):\n        """Navigate to a specific pose"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = goal_pose\n        \n        # Send navigation goal\n        self.nav_to_pose_client.wait_for_server()\n        future = self.nav_to_pose_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_result_callback)\n        \n        self.navigation_active = True\n        self.balance_state = "STABLE"\n    \n    def navigation_step(self):\n        """Main navigation control loop"""\n        if not self.navigation_active or self.balance_state != "STABLE":\n            return\n        \n        # Plan footstep trajectory based on global plan\n        footstep_plan = self.plan_footsteps()\n        \n        # Execute balance-aware locomotion\n        cmd_vel = self.generate_locomotion_command(footstep_plan)\n        self.cmd_vel_pub.publish(cmd_vel)\n        \n        # Monitor progress and safety\n        self.check_navigation_safety()\n    \n    def plan_footsteps(self):\n        """Plan footstep sequence for bipedal locomotion"""\n        # This is a simplified implementation\n        # In practice, this would use specialized footstep planners\n        footstep_sequence = []\n        \n        if self.current_pose and self.goal_pose:\n            # Calculate desired direction of movement\n            dx = self.goal_pose.position.x - self.current_pose.position.x\n            dy = self.goal_pose.position.y - self.current_pose.position.y\n            distance_to_goal = np.sqrt(dx*dx + dy*dy)\n            \n            # Generate footsteps toward goal\n            # The actual implementation would consider:\n            # - Robot kinematics\n            # - Obstacle avoidance\n            # - Step constraints (step length, width)\n            # - Balance maintenance\n            \n        return footstep_sequence\n    \n    def generate_locomotion_command(self, footstep_plan):\n        """Generate velocity command based on footstep plan"""\n        cmd = Twist()\n        \n        if len(footstep_plan) > 0:\n            # Calculate desired velocity based on next footsteps\n            # This would integrate with the robot\'s walking controller\n            cmd.linear.x = 0.3  # Forward velocity\n            cmd.angular.z = 0.1  # Turning rate\n            \n            # Ensure velocity commands are within humanoid constraints\n            cmd.linear.x = max(-0.5, min(0.5, cmd.linear.x))  # Limit linear velocity\n            cmd.angular.z = max(-0.7, min(0.7, cmd.angular.z))  # Limit angular velocity\n        \n        return cmd\n    \n    def check_navigation_safety(self):\n        """Check navigation safety and adjust behavior as needed"""\n        # Check for obstacles in path\n        # Check for balance stability\n        # Check for goal achievement\n        pass\n    \n    def safety_stop(self):\n        """Emergency stop for safety"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)  # Stop all motion\n        self.navigation_active = False\n        self.get_logger().warn(\'Safety stop activated - humanoid balance compromised!\')\n    \n    def update_navigation_state(self):\n        """Update navigation state based on current conditions"""\n        if self.current_pose and self.goal_pose:\n            # Calculate distance and angle to goal\n            dx = self.goal_pose.position.x - self.current_pose.position.x\n            dy = self.goal_pose.position.y - self.current_pose.position.y\n            distance = np.sqrt(dx*dx + dy*dy)\n            \n            if distance < 0.2:  # Goal tolerance\n                self.navigation_active = False\n                self.get_logger().info(\'Navigation goal reached\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidNavigationNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"path-execution-and-safety",children:"Path Execution and Safety"}),"\n",(0,i.jsx)(e.h3,{id:"humanoid-path-execution-challenges",children:"Humanoid Path Execution Challenges"}),"\n",(0,i.jsx)(e.p,{children:"Executing paths with humanoid robots requires consideration of:"}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Dynamic Constraints"}),": Humanoid robots must maintain dynamic balance while following paths, which limits their ability to follow arbitrary paths."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Turning Radius"}),": Due to bipedal locomotion, humanoid robots have specific turning characteristics that must be considered."]}),"\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Step Planning"}),": The path may need to be converted to a sequence of foot placements rather than a continuous trajectory."]}),"\n",(0,i.jsx)(e.h3,{id:"safety-mechanisms",children:"Safety Mechanisms"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robot navigation systems require robust safety mechanisms:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class SafetyManager(Node):\n    def __init__(self):\n        super().__init__(\'safety_manager\')\n        \n        # Publishers for safety commands\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/emergency_stop\', 10)\n        self.safety_status_pub = self.create_publisher(String, \'/safety_status\', 10)\n        \n        # Subscriptions for safety monitoring\n        self.imu_sub = self.create_subscription(Imu, \'/imu/data\', self.imu_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/scan\', self.scan_callback, 10)\n        \n        # Safety timer\n        self.safety_timer = self.create_timer(0.01, self.check_safety)  # 100Hz safety checks\n        \n        # Safety thresholds\n        self.tilt_threshold = 0.3  # Radians\n        self.collision_threshold = 0.3  # Meters\n        \n    def check_safety(self):\n        """Perform safety checks for humanoid navigation"""\n        # Check balance\n        if self.imu_data:\n            orientation = self.imu_data.orientation\n            tilt_angle = abs(orientation.z)  # Simplified tilt check\n            \n            if tilt_angle > self.tilt_threshold:\n                self.trigger_safety_stop("EXCESSIVE_TILT")\n        \n        # Check for immediate collision risk\n        if self.scan_data:\n            min_distance = min(self.scan_data.ranges)\n            if min_distance < self.collision_threshold:\n                self.trigger_safety_stop("COLLISION_IMMINENT")\n    \n    def trigger_safety_stop(self, reason):\n        """Trigger emergency stop and report reason"""\n        self.get_logger().error(f\'Safety stop triggered: {reason}\')\n        \n        # Publish emergency stop command\n        stop_msg = Bool()\n        stop_msg.data = True\n        self.emergency_stop_pub.publish(stop_msg)\n        \n        # Report safety status\n        status_msg = String()\n        status_msg.data = reason\n        self.safety_status_pub.publish(status_msg)\n        \n        # Trigger recovery behavior\n        self.initiate_recovery()\n    \n    def initiate_recovery(self):\n        """Initiate recovery behavior for the humanoid robot"""\n        # This would send commands to the robot\'s balance controller\n        # to bring it to a stable pose\n        pass\n'})}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-31-implement-vslam--nav2-integration",children:"Hands-on Exercise 3.1: Implement VSLAM + Nav2 Integration"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Set up Isaac ROS Visual SLAM with your humanoid robot model"}),"\n",(0,i.jsx)(e.li,{children:"Configure Nav2 parameters specifically for humanoid navigation"}),"\n",(0,i.jsx)(e.li,{children:"Integrate the VSLAM pose estimates with the Nav2 localization system"}),"\n",(0,i.jsx)(e.li,{children:"Set up a navigation goal and observe the robot's path execution"}),"\n",(0,i.jsx)(e.li,{children:"Validate that the robot can navigate through simple environments using VSLAM for localization"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-32-humanoid-navigation-with-safety",children:"Hands-on Exercise 3.2: Humanoid Navigation with Safety"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement the safety monitoring system in your navigation pipeline"}),"\n",(0,i.jsx)(e.li,{children:"Test navigation with safety constraints (e.g., stop if tilt exceeds threshold)"}),"\n",(0,i.jsx)(e.li,{children:"Create scenarios that test the robot's ability to handle obstacles"}),"\n",(0,i.jsx)(e.li,{children:"Validate the navigation performance and safety system in simulation"}),"\n",(0,i.jsx)(e.li,{children:"Document any issues with path execution and potential improvements"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"validation-checklist",children:"Validation Checklist"}),"\n",(0,i.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand the fundamentals of Visual SLAM for humanoid robotics"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can configure Isaac ROS Visual SLAM for humanoid applications"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand how to integrate VSLAM with the Nav2 navigation stack"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can configure Nav2 parameters for bipedal humanoid navigation"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have implemented a complete navigation system with safety mechanisms"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have validated navigation performance in simulation"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand the unique challenges of humanoid path execution"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have tested safety mechanisms in the navigation system"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter covered the integration of Isaac ROS Visual SLAM with the Nav2 navigation stack to enable autonomous navigation for humanoid robots. We explored the unique challenges of humanoid navigation, including bipedal locomotion, balance requirements, and safety considerations. We also examined how to configure the navigation system specifically for humanoid robots and implement safety mechanisms."}),"\n",(0,i.jsx)(e.p,{children:"The combination of Isaac ROS's GPU-accelerated VSLAM and Nav2's flexible navigation framework provides a robust solution for humanoid robot navigation. The safety-focused approach ensures that navigation commands are appropriate for the dynamic stability requirements of bipedal robots."}),"\n",(0,i.jsx)(e.p,{children:"With the navigation system in place, humanoid robots can now perceive their environment, build maps, and navigate safely through complex spaces - a critical capability for autonomous humanoid robot operation."})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>s,x:()=>l});var i=a(6540);const t={},o=i.createContext(t);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);