"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[839],{7543:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>l,toc:()=>r});var t=i(4848),o=i(8453);const s={sidebar_position:1},a="Module 4: Vision-Language-Action (VLA)",l={id:"content/modules/vla/intro",title:"Module 4: Vision-Language-Action (VLA)",description:"Overview",source:"@site/docs/content/modules/004-vla/intro.md",sourceDirName:"content/modules/004-vla",slug:"/content/modules/vla/intro",permalink:"/hackathon-textbook/docs/content/modules/vla/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/content/modules/004-vla/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"textbookSidebar",previous:{title:"Module 3 Validation Checklist",permalink:"/hackathon-textbook/docs/content/modules/ai-robot-brain/checklists/validation-checklist"},next:{title:"Chapter 1: Intro to Vision-Language-Action - VLA Concepts, LLM Role in Humanoid Control",permalink:"/hackathon-textbook/docs/content/modules/vla/chapter-1"}},c={},r=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"Hands-on Tasks",id:"hands-on-tasks",level:2}];function d(e){const n={h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This module integrates Large Language Models (LLMs) with robotics for language-driven actions. You'll learn to implement a pipeline that processes voice commands through Whisper transcription to cognitive planning and ROS 2 action execution."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand Vision-Language-Action (VLA) concepts and LLM integration"}),"\n",(0,t.jsx)(n.li,{children:"Learn to implement Whisper-based voice input processing"}),"\n",(0,t.jsx)(n.li,{children:"Design cognitive planning that maps natural language to ROS 2 actions"}),"\n",(0,t.jsx)(n.li,{children:"Execute a full autonomous humanoid task cycle in simulation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understanding of ROS 2 (Module 1)"}),"\n",(0,t.jsx)(n.li,{children:"Knowledge of simulation (Module 2)"}),"\n",(0,t.jsx)(n.li,{children:"AI/ML concepts (Module 3)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vision-Language-Action system architecture"}),"\n",(0,t.jsx)(n.li,{children:"Whisper for voice command transcription"}),"\n",(0,t.jsx)(n.li,{children:"Cognitive planning for action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Natural language to ROS 2 action mapping"}),"\n",(0,t.jsx)(n.li,{children:"Capstone: full autonomous task pipeline"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-tasks",children:"Hands-on Tasks"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will complete practical exercises to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement Whisper voice input processing"}),"\n",(0,t.jsx)(n.li,{children:"Convert natural language commands to ROS 2 action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Simulate the full flow: voice \u2192 plan \u2192 navigate \u2192 identify \u2192 manipulate"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);