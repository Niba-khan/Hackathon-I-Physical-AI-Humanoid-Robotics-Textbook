"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[210],{4450:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var t=i(4848),o=i(8453);const a={},r="Chapter 3: Unity High-Fidelity Rendering & Interaction - Realistic Rendering, Human-Robot Interaction Scenes, Gazebo-Unity Bridging",s={id:"content/modules/digital-twin/chapter-3",title:"Chapter 3: Unity High-Fidelity Rendering & Interaction - Realistic Rendering, Human-Robot Interaction Scenes, Gazebo-Unity Bridging",description:"Objectives",source:"@site/docs/content/modules/002-digital-twin/chapter-3.md",sourceDirName:"content/modules/002-digital-twin",slug:"/content/modules/digital-twin/chapter-3",permalink:"/hackathon-textbook/docs/content/modules/digital-twin/chapter-3",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/content/modules/002-digital-twin/chapter-3.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 2: Gazebo Simulation Fundamentals - Physics Engine Basics, Environment Setup, Sensor Integration",permalink:"/hackathon-textbook/docs/content/modules/digital-twin/chapter-2"},next:{title:"Module 2 Validation Checklist",permalink:"/hackathon-textbook/docs/content/modules/digital-twin/checklists/validation-checklist"}},l={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Introduction to Unity for Robotics",id:"introduction-to-unity-for-robotics",level:2},{value:"Unity Setup for Robotics Applications",id:"unity-setup-for-robotics-applications",level:2},{value:"1. Installing Unity Hub and Editor",id:"1-installing-unity-hub-and-editor",level:3},{value:"2. Essential Unity Packages for Robotics",id:"2-essential-unity-packages-for-robotics",level:3},{value:"Creating Realistic Rendering Scenarios",id:"creating-realistic-rendering-scenarios",level:2},{value:"1. Setting Up a Robot Scene",id:"1-setting-up-a-robot-scene",level:3},{value:"Light Setup for Realistic Rendering",id:"light-setup-for-realistic-rendering",level:4},{value:"Material System for Robot Parts",id:"material-system-for-robot-parts",level:4},{value:"2. Environment Creation",id:"2-environment-creation",level:3},{value:"Using ProBuilder for Environment Creation",id:"using-probuilder-for-environment-creation",level:4},{value:"Importing Real-World Environments",id:"importing-real-world-environments",level:4},{value:"3. Rendering Pipeline Considerations",id:"3-rendering-pipeline-considerations",level:3},{value:"Universal Render Pipeline (URP) - Recommended for Robotics",id:"universal-render-pipeline-urp---recommended-for-robotics",level:4},{value:"High Definition Render Pipeline (HDRP) - For Maximum Quality",id:"high-definition-render-pipeline-hdrp---for-maximum-quality",level:4},{value:"Human-Robot Interaction Scenes",id:"human-robot-interaction-scenes",level:2},{value:"1. Interaction Design Principles",id:"1-interaction-design-principles",level:3},{value:"Safety Zones",id:"safety-zones",level:4},{value:"Intuitive Controls",id:"intuitive-controls",level:4},{value:"Natural Motion Representation",id:"natural-motion-representation",level:4},{value:"2. Implementing Interaction Systems",id:"2-implementing-interaction-systems",level:3},{value:"Teleoperation Interface",id:"teleoperation-interface",level:4},{value:"Safety Visualization",id:"safety-visualization",level:4},{value:"Gazebo-Unity Bridging Strategies",id:"gazebo-unity-bridging-strategies",level:2},{value:"1. Direct Bridge Approach",id:"1-direct-bridge-approach",level:3},{value:"Unity Robotics Hub Setup",id:"unity-robotics-hub-setup",level:4},{value:"2. Custom Middleware Bridge",id:"2-custom-middleware-bridge",level:3},{value:"3. File-Based Synchronization",id:"3-file-based-synchronization",level:3},{value:"Creating Interactive Scenes",id:"creating-interactive-scenes",level:2},{value:"1. Scene Hierarchy and Organization",id:"1-scene-hierarchy-and-organization",level:3},{value:"2. State Visualization",id:"2-state-visualization",level:3},{value:"Performance Optimization for Real-time Rendering",id:"performance-optimization-for-real-time-rendering",level:2},{value:"1. Level of Detail (LOD)",id:"1-level-of-detail-lod",level:3},{value:"2. Occlusion Culling",id:"2-occlusion-culling",level:3},{value:"Hands-on Exercise 3.1: Create a Unity Human-Robot Interaction Scene",id:"hands-on-exercise-31-create-a-unity-human-robot-interaction-scene",level:2},{value:"Hands-on Exercise 3.2: Design a Gazebo-Unity Bridge Architecture",id:"hands-on-exercise-32-design-a-gazebo-unity-bridge-architecture",level:2},{value:"Validation Checklist",id:"validation-checklist",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-3-unity-high-fidelity-rendering--interaction---realistic-rendering-human-robot-interaction-scenes-gazebo-unity-bridging",children:"Chapter 3: Unity High-Fidelity Rendering & Interaction - Realistic Rendering, Human-Robot Interaction Scenes, Gazebo-Unity Bridging"}),"\n",(0,t.jsx)(n.h2,{id:"objectives",children:"Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand Unity's capabilities for high-fidelity robotics visualization"}),"\n",(0,t.jsx)(n.li,{children:"Create realistic rendering scenarios for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Implement human-robot interaction scenes in Unity"}),"\n",(0,t.jsx)(n.li,{children:"Explore Gazebo-Unity bridging strategies for complete digital twin functionality"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction-to-unity-for-robotics",children:"Introduction to Unity for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Unity is a powerful real-time 3D development platform that provides high-fidelity rendering capabilities essential for digital twin applications in robotics. For humanoid robotics specifically, Unity offers:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Photorealistic Rendering"}),": Advanced lighting, shadows, and materials"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time Performance"}),": High frame rates for interactive applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extensive Asset Library"}),": Models, environments, and effects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scripting Capabilities"}),": C# scripting for custom behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"XR Support"}),": Virtual and augmented reality capabilities"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"While Gazebo excels at physics simulation, Unity excels at visual rendering, making the combination ideal for complete digital twin solutions."}),"\n",(0,t.jsx)(n.h2,{id:"unity-setup-for-robotics-applications",children:"Unity Setup for Robotics Applications"}),"\n",(0,t.jsx)(n.h3,{id:"1-installing-unity-hub-and-editor",children:"1. Installing Unity Hub and Editor"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Download Unity Hub from ",(0,t.jsx)(n.a,{href:"https://unity.com/",children:"https://unity.com/"})]}),"\n",(0,t.jsx)(n.li,{children:"Install the latest LTS (Long Term Support) version"}),"\n",(0,t.jsxs)(n.li,{children:["Install additional modules for robotics:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Universal Render Pipeline (URP) or High Definition Render Pipeline (HDRP)"}),"\n",(0,t.jsx)(n.li,{children:"Physics libraries"}),"\n",(0,t.jsx)(n.li,{children:"Input System package"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-essential-unity-packages-for-robotics",children:"2. Essential Unity Packages for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Add these packages through the Package Manager:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Universal Render Pipeline (URP)"}),": For optimized rendering"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physics"}),": For collision detection and basic physics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input System"}),": For handling user interactions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ProBuilder"}),": For quick environment prototyping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ProGrids"}),": For precise object placement"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"creating-realistic-rendering-scenarios",children:"Creating Realistic Rendering Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"1-setting-up-a-robot-scene",children:"1. Setting Up a Robot Scene"}),"\n",(0,t.jsx)(n.h4,{id:"light-setup-for-realistic-rendering",children:"Light Setup for Realistic Rendering"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// In a script, or via the Unity Editor\n// Create a Directional Light to simulate sunlight\n// Position it to match Gazebo's sun direction: -0.5, -0.1, -0.9\n// Set Intensity to 1.0 and Color to a warm white\n"})}),"\n",(0,t.jsx)(n.h4,{id:"material-system-for-robot-parts",children:"Material System for Robot Parts"}),"\n",(0,t.jsx)(n.p,{children:"Unity's material system allows for realistic robot rendering:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'// Example: Creating a metallic material for robot parts\nShader standardShader = Shader.Find("Universal Render Pipeline/Lit");\nMaterial robotMetalMat = new Material(standardShader);\nrobotMetalMat.SetColor("_BaseColor", Color.grey);\nrobotMetalMat.SetFloat("_Metallic", 0.8f); // Highly metallic\nrobotMetalMat.SetFloat("_Smoothness", 0.6f); // Slightly smooth but not mirror-like\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-environment-creation",children:"2. Environment Creation"}),"\n",(0,t.jsx)(n.p,{children:"Create realistic environments for your humanoid robot:"}),"\n",(0,t.jsx)(n.h4,{id:"using-probuilder-for-environment-creation",children:"Using ProBuilder for Environment Creation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Create a new GameObject"}),"\n",(0,t.jsx)(n.li,{children:"Add ProBuilder Shape component (Cube for ground, walls, etc.)"}),"\n",(0,t.jsx)(n.li,{children:"Edit the shape using ProBuilder tools"}),"\n",(0,t.jsx)(n.li,{children:"Apply appropriate materials"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"importing-real-world-environments",children:"Importing Real-World Environments"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Use Google Earth data or satellite imagery"}),"\n",(0,t.jsx)(n.li,{children:"Import 3D models from SketchUp or other CAD software"}),"\n",(0,t.jsx)(n.li,{children:"Use Unity's Terrain tools for large outdoor areas"}),"\n",(0,t.jsx)(n.li,{children:"Apply realistic textures and lighting"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-rendering-pipeline-considerations",children:"3. Rendering Pipeline Considerations"}),"\n",(0,t.jsx)(n.p,{children:"For robotics applications, choose the appropriate rendering pipeline:"}),"\n",(0,t.jsx)(n.h4,{id:"universal-render-pipeline-urp---recommended-for-robotics",children:"Universal Render Pipeline (URP) - Recommended for Robotics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Better performance for real-time applications"}),"\n",(0,t.jsx)(n.li,{children:"Suitable for rendering multiple robots simultaneously"}),"\n",(0,t.jsx)(n.li,{children:"Good balance between quality and performance"}),"\n",(0,t.jsx)(n.li,{children:"Lower system requirements"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"high-definition-render-pipeline-hdrp---for-maximum-quality",children:"High Definition Render Pipeline (HDRP) - For Maximum Quality"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Photorealistic rendering quality"}),"\n",(0,t.jsx)(n.li,{children:"Advanced lighting and shadow techniques"}),"\n",(0,t.jsx)(n.li,{children:"Higher system requirements"}),"\n",(0,t.jsx)(n.li,{children:"Better for marketing/final visualization"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example URP setup in C#:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.Rendering;\nusing UnityEngine.Rendering.Universal;\n\npublic class RobotRendererSetup : MonoBehaviour\n{\n    void Start()\n    {\n        // Configure URP settings for robot rendering\n        var universalRenderPipelineAsset = (UniversalRenderPipelineAsset)GraphicsSettings.renderPipelineAsset;\n        \n        if (universalRenderPipelineAsset != null)\n        {\n            // Set up rendering for robot models\n            ConfigureForRobotRendering(universalRenderPipelineAsset);\n        }\n    }\n    \n    void ConfigureForRobotRendering(UniversalRenderPipelineAsset asset)\n    {\n        // Adjust rendering settings for optimal robot visualization\n        // This is pseudocode; actual URP configuration requires different approach\n        Debug.Log("URP configured for robot rendering");\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"human-robot-interaction-scenes",children:"Human-Robot Interaction Scenes"}),"\n",(0,t.jsx)(n.h3,{id:"1-interaction-design-principles",children:"1. Interaction Design Principles"}),"\n",(0,t.jsx)(n.p,{children:"When designing human-robot interaction scenes in Unity, consider:"}),"\n",(0,t.jsx)(n.h4,{id:"safety-zones",children:"Safety Zones"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visualize robot's workspace and safety boundaries"}),"\n",(0,t.jsx)(n.li,{children:"Use transparent overlays to indicate danger zones"}),"\n",(0,t.jsx)(n.li,{children:"Highlight robot's reach and operational area"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"intuitive-controls",children:"Intuitive Controls"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design interfaces that non-technical users can understand"}),"\n",(0,t.jsx)(n.li,{children:"Provide clear feedback when interacting with the robot"}),"\n",(0,t.jsx)(n.li,{children:"Use visual indicators for robot's current state"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"natural-motion-representation",children:"Natural Motion Representation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Show robot's intention through subtle animations"}),"\n",(0,t.jsx)(n.li,{children:"Visualize planned paths before execution"}),"\n",(0,t.jsx)(n.li,{children:"Highlight robot's attention/focus points"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-implementing-interaction-systems",children:"2. Implementing Interaction Systems"}),"\n",(0,t.jsx)(n.h4,{id:"teleoperation-interface",children:"Teleoperation Interface"}),"\n",(0,t.jsx)(n.p,{children:"Create a UI system for remote robot operation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\n\npublic class TeleoperationInterface : MonoBehaviour\n{\n    [Header("Robot Control")]\n    public Transform robotBase;\n    public Transform robotHead;\n    \n    [Header("UI Elements")]\n    public Slider linearVelocitySlider;\n    public Slider angularVelocitySlider;\n    public Button startButton;\n    public Button stopButton;\n    \n    [Header("Visualization")]\n    public LineRenderer pathRenderer;\n    \n    // Update robot visualization based on inputs\n    void Update()\n    {\n        if (robotBase != null)\n        {\n            // Update robot position/rotation visualization\n            float linearVel = linearVelocitySlider.value;\n            float angularVel = angularVelocitySlider.value;\n            \n            // Visualize robot\'s intent\n            VisualizePath(linearVel, angularVel);\n        }\n    }\n    \n    void VisualizePath(float linearVel, float angularVel)\n    {\n        // Draw the planned path based on current velocity commands\n        // This is where you\'d implement path visualization\n        if (pathRenderer != null)\n        {\n            // Calculate and display the robot\'s projected path\n            Vector3[] pathPoints = CalculatePath(linearVel, angularVel);\n            pathRenderer.positionCount = pathPoints.Length;\n            pathRenderer.SetPositions(pathPoints);\n        }\n    }\n    \n    Vector3[] CalculatePath(float linearVel, float angularVel)\n    {\n        // Simplified path calculation\n        Vector3[] points = new Vector3[10];\n        Vector3 startPos = robotBase.position;\n        Vector3 direction = robotBase.forward;\n        \n        for (int i = 0; i < points.Length; i++)\n        {\n            points[i] = startPos + direction * linearVel * i * 0.1f;\n            direction = Quaternion.Euler(0, angularVel * i * 0.1f, 0) * robotBase.forward;\n        }\n        \n        return points;\n    }\n    \n    public void OnStartRobot()\n    {\n        // Send start command to robot\n        Debug.Log("Start robot command sent");\n    }\n    \n    public void OnStopRobot()\n    {\n        // Send stop command to robot\n        Debug.Log("Stop robot command sent");\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"safety-visualization",children:"Safety Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Highlight important safety elements:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class SafetyZoneVisualizer : MonoBehaviour\n{\n    [Header("Safety Parameters")]\n    public float safeDistance = 2.0f; // Safe distance from robot\n    public float dangerDistance = 0.5f; // Dangerous distance\n    public LayerMask obstacleLayer;\n    \n    [Header("Visualization")]\n    public GameObject safeZoneIndicator;\n    public GameObject dangerZoneIndicator;\n    \n    void Update()\n    {\n        VisualizeSafetyZones();\n    }\n    \n    void VisualizeSafetyZones()\n    {\n        // Draw spheres to represent safe and danger distances\n        if (safeZoneIndicator != null)\n        {\n            safeZoneIndicator.transform.localScale = Vector3.one * safeDistance * 2;\n            safeZoneIndicator.SetActive(true);\n        }\n        \n        if (dangerZoneIndicator != null)\n        {\n            dangerZoneIndicator.transform.localScale = Vector3.one * dangerDistance * 2;\n            dangerZoneIndicator.SetActive(true);\n        }\n    }\n    \n    // Check for obstacles in safety zones\n    bool IsPathClear(Vector3 start, Vector3 end, float radius)\n    {\n        // Raycast or sphere cast to check for obstacles\n        return !Physics.SphereCast(start, radius, (end - start).normalized, \n                                  Vector3.Distance(start, end), obstacleLayer);\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"gazebo-unity-bridging-strategies",children:"Gazebo-Unity Bridging Strategies"}),"\n",(0,t.jsx)(n.p,{children:"A complete digital twin requires synchronization between Gazebo (physics simulation) and Unity (visual rendering). Here are the main approaches:"}),"\n",(0,t.jsx)(n.h3,{id:"1-direct-bridge-approach",children:"1. Direct Bridge Approach"}),"\n",(0,t.jsx)(n.p,{children:"Use the Unity Robotics Hub and Gazebo integration tools:"}),"\n",(0,t.jsx)(n.h4,{id:"unity-robotics-hub-setup",children:"Unity Robotics Hub Setup"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Install Unity Robotics Hub from Unity Asset Store"}),"\n",(0,t.jsx)(n.li,{children:"Add ROS-TCP-Connector package"}),"\n",(0,t.jsx)(n.li,{children:"Configure network settings"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example synchronization script:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing RosMessageTypes.Sensor;\nusing RosMessageTypes.Geometry;\nusing Unity.Robotics.ROSTCPConnector;\n\npublic class GazeboUnityBridge : MonoBehaviour\n{\n    [Header("ROS Connection")]\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n    \n    [Header("Robot Components")]\n    public Transform robotBase;\n    public Transform[] robotJoints;\n    public Transform[] robotLinks;\n    \n    private ROSConnection ros;\n    private string jointTopic = "/joint_states";\n    private string tfTopic = "/tf";\n    \n    void Start()\n    {\n        // Get reference to ROS connection\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<JointStateMsg>(jointTopic);\n        \n        // Subscribe to joint state updates\n        ros.Subscribe<JointStateMsg>("joint_states", UpdateRobotFromGazebo);\n    }\n    \n    void UpdateRobotFromGazebo(JointStateMsg jointMsg)\n    {\n        // Synchronize robot position and joint angles from Gazebo\n        if (robotJoints.Length == jointMsg.position.Length)\n        {\n            for (int i = 0; i < robotJoints.Length; i++)\n            {\n                // Update joint rotation based on Gazebo simulation\n                robotJoints[i].localRotation = Quaternion.Euler(0, 0, (float)(jointMsg.position[i] * Mathf.Rad2Deg));\n            }\n        }\n    }\n    \n    void SendCommandsToGazebo()\n    {\n        // Send joint commands to Gazebo\n        JointStateMsg cmd = new JointStateMsg();\n        cmd.name = new string[] { "joint1", "joint2", "joint3" };\n        cmd.position = new double[] { 0.0, 0.0, 0.0 }; // Example positions\n        cmd.velocity = new double[] { 0.0, 0.0, 0.0 };\n        cmd.effort = new double[] { 0.0, 0.0, 0.0 };\n        \n        ros.Publish(jointTopic, cmd);\n    }\n    \n    void Update()\n    {\n        // Continuously send sensor data to Gazebo\n        // Update visuals based on Gazebo state\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-custom-middleware-bridge",children:"2. Custom Middleware Bridge"}),"\n",(0,t.jsx)(n.p,{children:"Develop a custom bridge using technologies like ZeroMQ:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'// Simplified example - would require ZeroMQ libraries and custom server\nusing UnityEngine;\nusing System.Collections;\n\npublic class CustomBridge : MonoBehaviour\n{\n    [Header("Bridge Settings")]\n    public string bridgeAddress = "tcp://localhost:5555";\n    \n    // This is a conceptual example\n    // Actual implementation would require specific bridge technology\n    \n    IEnumerator ConnectToBridge()\n    {\n        // Connect to the bridge service\n        // This would handle communication with a custom bridge server\n        yield return null;\n    }\n    \n    void SynchronizeWithGazebo()\n    {\n        // Send/receive data to keep Unity and Gazebo in sync\n        // This could be done via JSON, Protocol Buffers, or custom formats\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-file-based-synchronization",children:"3. File-Based Synchronization"}),"\n",(0,t.jsx)(n.p,{children:"Use file exchanges for less real-time applications:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.IO;\nusing System.Collections.Generic;\n\npublic class FileBasedSync : MonoBehaviour\n{\n    [Header("File Sync Settings")]\n    public string dataPath = "Assets/StreamingAssets/";\n    public float syncInterval = 0.1f;\n    \n    private float lastSyncTime;\n    \n    void Update()\n    {\n        if (Time.time - lastSyncTime > syncInterval)\n        {\n            SyncWithGazebo();\n            lastSyncTime = Time.time;\n        }\n    }\n    \n    void SyncWithGazebo()\n    {\n        // Read robot state from shared file\n        string robotStateFile = Path.Combine(dataPath, "robot_state.json");\n        if (File.Exists(robotStateFile))\n        {\n            string json = File.ReadAllText(robotStateFile);\n            RobotState state = JsonUtility.FromJson<RobotState>(json);\n            \n            // Update Unity visualization based on robot state\n            UpdateRobotVisualization(state);\n        }\n    }\n    \n    void UpdateRobotVisualization(RobotState state)\n    {\n        if (robotBase != null)\n        {\n            robotBase.position = new Vector3((float)state.position.x, (float)state.position.y, (float)state.position.z);\n            robotBase.rotation = Quaternion.Euler((float)state.orientation.x, (float)state.orientation.y, (float)state.orientation.z);\n        }\n    }\n}\n\n[System.Serializable]\npublic class RobotState\n{\n    public Vector3D position;\n    public Vector3D orientation;\n    public double[] jointPositions;\n    public double[] jointVelocities;\n}\n\n[System.Serializable]\npublic class Vector3D\n{\n    public double x, y, z;\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"creating-interactive-scenes",children:"Creating Interactive Scenes"}),"\n",(0,t.jsx)(n.h3,{id:"1-scene-hierarchy-and-organization",children:"1. Scene Hierarchy and Organization"}),"\n",(0,t.jsx)(n.p,{children:"Create a well-organized scene structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"RobotDigitalTwin\n\u251c\u2500\u2500 Environment\n\u2502   \u251c\u2500\u2500 Ground\n\u2502   \u251c\u2500\u2500 Walls\n\u2502   \u251c\u2500\u2500 Obstacles\n\u2502   \u2514\u2500\u2500 Lighting\n\u251c\u2500\u2500 Robot\n\u2502   \u251c\u2500\u2500 RobotBase\n\u2502   \u2502   \u251c\u2500\u2500 Head\n\u2502   \u2502   \u251c\u2500\u2500 Torso\n\u2502   \u2502   \u251c\u2500\u2500 LeftArm\n\u2502   \u2502   \u251c\u2500\u2500 RightArm\n\u2502   \u2502   \u251c\u2500\u2500 LeftLeg\n\u2502   \u2502   \u2514\u2500\u2500 RightLeg\n\u2502   \u2514\u2500\u2500 Sensors\n\u2502       \u251c\u2500\u2500 IMU\n\u2502       \u251c\u2500\u2500 Cameras\n\u2502       \u2514\u2500\u2500 LiDAR\n\u2514\u2500\u2500 UI\n    \u251c\u2500\u2500 ControlPanel\n    \u251c\u2500\u2500 SafetyZones\n    \u2514\u2500\u2500 PathVisualization\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-state-visualization",children:"2. State Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Show the robot's internal state and intentions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\n\npublic class RobotStateVisualizer : MonoBehaviour\n{\n    [Header("State Display")]\n    public Text stateText;           // Current state (IDLE, NAVIGATING, etc.)\n    public Image batteryFill;        // Battery level visualization\n    public Image connectionIndicator; // Connection status\n    public GameObject[] pathPoints;   // Planned path visualization\n    \n    [Header("Target Elements")]\n    public GameObject targetIndicator; // Current goal location\n    public Color idleColor = Color.gray;\n    public Color activeColor = Color.green;\n    public Color errorColor = Color.red;\n    \n    void Update()\n    {\n        UpdateStateDisplay();\n        UpdateTargetVisualization();\n    }\n    \n    void UpdateStateDisplay()\n    {\n        // Update based on robot state\n        // This would typically come from ROS or simulation state\n        string robotState = GetRobotState(); // Pseudo-implementation\n        stateText.text = $"State: {robotState}";\n        \n        // Update visual indicators\n        UpdateBatteryLevel();\n        UpdateConnectionStatus();\n    }\n    \n    string GetRobotState()\n    {\n        // In a real implementation, this would get the state from ROS topics\n        // For simulation purposes\n        return "NAVIGATING";\n    }\n    \n    void UpdateBatteryLevel()\n    {\n        // Update battery visualization (0-100%)\n        float batteryLevel = 85.0f; // Example value\n        batteryFill.fillAmount = batteryLevel / 100.0f;\n        \n        // Change color based on battery level\n        if (batteryLevel < 20)\n            batteryFill.color = Color.red;\n        else if (batteryLevel < 50)\n            batteryFill.color = Color.yellow;\n        else\n            batteryFill.color = Color.green;\n    }\n    \n    void UpdateConnectionStatus()\n    {\n        // Update connection indicator\n        // This would check for ROS connection or simulation status\n        bool isConnected = true; // Example value\n        connectionIndicator.color = isConnected ? Color.green : Color.red;\n    }\n    \n    void UpdateTargetVisualization()\n    {\n        // Show current target/destination\n        if (targetIndicator != null)\n        {\n            // Position the target indicator at the robot\'s current goal\n            // In a real implementation, this would come from navigation topics\n            Vector3 targetPos = new Vector3(5, 0, 5); // Example position\n            targetIndicator.transform.position = targetPos;\n            targetIndicator.SetActive(true);\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization-for-real-time-rendering",children:"Performance Optimization for Real-time Rendering"}),"\n",(0,t.jsx)(n.h3,{id:"1-level-of-detail-lod",children:"1. Level of Detail (LOD)"}),"\n",(0,t.jsx)(n.p,{children:"Implement LOD for robot models based on distance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\n\n[RequireComponent(typeof(MeshRenderer))]\npublic class RobotLOD : MonoBehaviour\n{\n    [Header("LOD Settings")]\n    [Range(0.0f, 1.0f)]\n    public float lodDistance = 0.5f; // Fraction of screen space at which to switch LOD\n    \n    [Header("LOD Models")]\n    public GameObject highDetailModel;\n    public GameObject lowDetailModel;\n    \n    private Camera mainCamera;\n    private float screenPercentage;\n    \n    void Start()\n    {\n        mainCamera = Camera.main;\n        screenPercentage = lodDistance;\n    }\n    \n    void Update()\n    {\n        if (mainCamera != null)\n        {\n            UpdateLOD();\n        }\n    }\n    \n    void UpdateLOD()\n    {\n        // Calculate the robot\'s size in screen space\n        float distance = Vector3.Distance(transform.position, mainCamera.transform.position);\n        float objectScreenSize = CalculateScreenSize(distance);\n        \n        if (objectScreenSize < screenPercentage)\n        {\n            // Use low-detail model\n            if (highDetailModel != null) highDetailModel.SetActive(false);\n            if (lowDetailModel != null) lowDetailModel.SetActive(true);\n        }\n        else\n        {\n            // Use high-detail model\n            if (lowDetailModel != null) lowDetailModel.SetActive(false);\n            if (highDetailModel != null) highDetailModel.SetActive(true);\n        }\n    }\n    \n    float CalculateScreenSize(float distance)\n    {\n        // Calculate the size of the object in screen space\n        // This is a simplified calculation\n        float objectHeight = 1.0f; // Estimated robot height\n        float objectSize = 2.0f * Mathf.Atan(objectHeight / (2.0f * distance));\n        return objectSize / (2.0f * Mathf.Tan(mainCamera.fieldOfView * Mathf.Deg2Rad / 2.0f));\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-occlusion-culling",children:"2. Occlusion Culling"}),"\n",(0,t.jsx)(n.p,{children:"Use Unity's occlusion culling for large environments:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'Mark static objects as "Occluder Static" or "Occludee Static"'}),"\n",(0,t.jsx)(n.li,{children:"Build occlusion culling data via Window > Rendering > Occlusion Culling"}),"\n",(0,t.jsx)(n.li,{children:"Use this feature especially for indoor environments with walls/doors"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise-31-create-a-unity-human-robot-interaction-scene",children:"Hands-on Exercise 3.1: Create a Unity Human-Robot Interaction Scene"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a new Unity scene with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A simple humanoid robot model (or use the one from previous exercises)"}),"\n",(0,t.jsx)(n.li,{children:"An indoor environment with furniture and obstacles"}),"\n",(0,t.jsx)(n.li,{children:"Proper lighting setup"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement the following interaction elements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A UI panel with teleoperation controls (move forward, turn, stop)"}),"\n",(0,t.jsx)(n.li,{children:"Visual indicators for robot's current state"}),"\n",(0,t.jsx)(n.li,{children:"Path visualization showing where the robot plans to move"}),"\n",(0,t.jsx)(n.li,{children:"Safety zone indicators around the robot"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a basic synchronization system with these elements:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot position updates (simulated, not connected to Gazebo yet)"}),"\n",(0,t.jsx)(n.li,{children:"Joint angle visualization"}),"\n",(0,t.jsx)(n.li,{children:"Basic sensor visualization (e.g., LiDAR points if creating them)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Test the scene for performance and usability"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise-32-design-a-gazebo-unity-bridge-architecture",children:"Hands-on Exercise 3.2: Design a Gazebo-Unity Bridge Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Create a document outlining your approach to bridging Gazebo and Unity:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Identify what data needs to flow between systems"}),"\n",(0,t.jsx)(n.li,{children:"Choose a bridge implementation approach (direct, custom middleware, file-based)"}),"\n",(0,t.jsx)(n.li,{children:"Design the data structures that will be synchronized"}),"\n",(0,t.jsx)(n.li,{children:"Consider real-time performance requirements for your application"}),"\n",(0,t.jsx)(n.li,{children:"Plan for handling delays and potential data desynchronization"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"validation-checklist",children:"Validation Checklist"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I understand Unity's capabilities for high-fidelity robotics visualization"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I can create realistic rendering scenarios for humanoid robots"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I have implemented human-robot interaction scenes in Unity"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I understand different Gazebo-Unity bridging strategies"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I have created an interactive Unity scene with proper performance optimization"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","I have designed a bridge architecture between Gazebo and Unity"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered the advanced aspects of digital twin technology using Unity for high-fidelity visualization and human-robot interaction. We explored Unity setup for robotics, realistic rendering techniques, human-robot interaction design principles, and various approaches to bridging Gazebo and Unity for complete digital twin functionality."}),"\n",(0,t.jsx)(n.p,{children:"The combination of Gazebo's physics simulation and Unity's high-fidelity rendering creates a powerful platform for digital twin applications in humanoid robotics. When properly implemented, these systems enable safe, efficient development and testing of complex robotic behaviors before deployment to physical robots."}),"\n",(0,t.jsx)(n.p,{children:"With this understanding of digital twin technology, we now have the foundation for creating comprehensive simulation environments that can significantly reduce development time and risk for humanoid robotics projects."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);