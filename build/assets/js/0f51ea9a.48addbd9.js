"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[515],{6523:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var i=t(4848),a=t(8453);const o={},s="Chapter 2: Perception & Synthetic Data - Photorealistic Scenes, Sensor Simulation, Dataset Generation",r={id:"content/modules/ai-robot-brain/chapter-2",title:"Chapter 2: Perception & Synthetic Data - Photorealistic Scenes, Sensor Simulation, Dataset Generation",description:"Objectives",source:"@site/docs/content/modules/003-ai-robot-brain/chapter-2.md",sourceDirName:"content/modules/003-ai-robot-brain",slug:"/content/modules/ai-robot-brain/chapter-2",permalink:"/docs/content/modules/ai-robot-brain/chapter-2",draft:!1,unlisted:!1,editUrl:"https://github.com/Niba-khan/Hackathon-I-Physical-AI-Humanoid-Robotics-Textbook/tree/main/docs/content/modules/003-ai-robot-brain/chapter-2.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Chapter 1: Intro to NVIDIA Isaac\u2122 - Overview, Humanoid Capabilities, ROS 2 Integration",permalink:"/docs/content/modules/ai-robot-brain/chapter-1"},next:{title:"Chapter 3: Navigation & Path Planning - Isaac ROS VSLAM, Nav2 Integration, Humanoid Path Execution",permalink:"/docs/content/modules/ai-robot-brain/chapter-3"}},l={},c=[{value:"Objectives",id:"objectives",level:2},{value:"Introduction to Perception in Humanoid Robotics",id:"introduction-to-perception-in-humanoid-robotics",level:2},{value:"Creating Photorealistic Scenes in Isaac Sim",id:"creating-photorealistic-scenes-in-isaac-sim",level:2},{value:"Scene Composition with USD",id:"scene-composition-with-usd",level:3},{value:"Setting up a Photorealistic Environment",id:"setting-up-a-photorealistic-environment",level:3},{value:"Material and Texturing for Realism",id:"material-and-texturing-for-realism",level:3},{value:"Sensor Simulation in Isaac Sim",id:"sensor-simulation-in-isaac-sim",level:2},{value:"Camera Simulation",id:"camera-simulation",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"IMU Simulation",id:"imu-simulation",level:3},{value:"Synthetic Data Generation Workflow",id:"synthetic-data-generation-workflow",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Data Collection Pipeline",id:"data-collection-pipeline",level:3},{value:"Generating Training Datasets",id:"generating-training-datasets",level:2},{value:"Classification Dataset",id:"classification-dataset",level:3},{value:"Detection Dataset",id:"detection-dataset",level:3},{value:"Validation of Synthetic Data Quality",id:"validation-of-synthetic-data-quality",level:2},{value:"Comparing with Real Data",id:"comparing-with-real-data",level:3},{value:"Quality Assessment Techniques",id:"quality-assessment-techniques",level:3},{value:"Hands-on Exercise 2.1: Create a Perception Scene",id:"hands-on-exercise-21-create-a-perception-scene",level:2},{value:"Hands-on Exercise 2.2: Generate a Perception Dataset",id:"hands-on-exercise-22-generate-a-perception-dataset",level:2},{value:"Validation Checklist",id:"validation-checklist",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-2-perception--synthetic-data---photorealistic-scenes-sensor-simulation-dataset-generation",children:"Chapter 2: Perception & Synthetic Data - Photorealistic Scenes, Sensor Simulation, Dataset Generation"}),"\n",(0,i.jsx)(e.h2,{id:"objectives",children:"Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand perception systems in humanoid robotics using Isaac Sim"}),"\n",(0,i.jsx)(e.li,{children:"Create photorealistic scenes with accurate physics in Isaac Sim"}),"\n",(0,i.jsx)(e.li,{children:"Implement sensor simulation for realistic data generation"}),"\n",(0,i.jsx)(e.li,{children:"Generate synthetic datasets for training perception models"}),"\n",(0,i.jsx)(e.li,{children:"Validate perception outputs against ground truth"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-perception-in-humanoid-robotics",children:"Introduction to Perception in Humanoid Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Perception is fundamental to humanoid robotics - it enables robots to understand and interact with their environment. For humanoid robots, perception systems must process complex visual and sensory information in real-time to support navigation, manipulation, object recognition, and human interaction."}),"\n",(0,i.jsx)(e.p,{children:"Isaac Sim provides a comprehensive platform for developing and testing perception systems in photorealistic environments with accurate physics simulation. This enables the generation of high-quality synthetic data that can be used to train perception models before deployment on physical robots."}),"\n",(0,i.jsx)(e.h2,{id:"creating-photorealistic-scenes-in-isaac-sim",children:"Creating Photorealistic Scenes in Isaac Sim"}),"\n",(0,i.jsx)(e.h3,{id:"scene-composition-with-usd",children:"Scene Composition with USD"}),"\n",(0,i.jsx)(e.p,{children:"Isaac Sim uses Universal Scene Description (USD) as its core scene representation format. USD enables:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Scalable scene composition"}),": Build complex scenes from modular components"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Collaborative workflows"}),": Multiple developers can work on different parts of a scene"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Version control"}),": Track changes to scenes over time"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Interoperability"}),": Import/export scenes to/from other tools"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"setting-up-a-photorealistic-environment",children:"Setting up a Photorealistic Environment"}),"\n",(0,i.jsx)(e.p,{children:"Let's create a photorealistic indoor environment for humanoid robot perception training:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example: Creating a photorealistic room in Isaac Sim using Python API\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import create_primitive\nfrom omni.isaac.sensor import Camera\nfrom pxr import Gf\nimport numpy as np\n\nclass PhotorealisticEnvironment:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.setup_scene()\n    \n    def setup_scene(self):\n        # Add a ground plane\n        create_primitive(\n            prim_path="/World/GroundPlane",\n            primitive_type="Plane",\n            position=np.array([0, 0, 0]),\n            orientation=np.array([0, 0, 0, 1]),\n            scale=np.array([10, 10, 1])\n        )\n        \n        # Add walls\n        create_primitive(\n            prim_path="/World/Wall1",\n            primitive_type="Cube",\n            position=np.array([0, -5, 1.5]),\n            orientation=np.array([0, 0, 0, 1]),\n            scale=np.array([10, 0.1, 3])\n        )\n        \n        # Add furniture\n        create_primitive(\n            prim_path="/World/Table",\n            primitive_type="Cube",\n            position=np.array([2, 0, 0.4]),\n            orientation=np.array([0, 0, 0, 1]),\n            scale=np.array([1.5, 0.8, 0.8])\n        )\n        \n        # Add objects for perception training\n        self.add_objects_for_perception()\n        \n        # Setup lighting\n        self.setup_lighting()\n    \n    def add_objects_for_perception(self):\n        """Add objects that will be used for perception training"""\n        # Add various objects at different positions\n        objects = [\n            {"name": "cube", "type": "Cube", "pos": [1, 1, 0.2], "size": [0.2, 0.2, 0.2]},\n            {"name": "sphere", "type": "Sphere", "pos": [-1, 0.5, 0.2], "size": [0.2, 0.2, 0.2]},\n            {"name": "cylinder", "type": "Cylinder", "pos": [0, -1, 0.2], "size": [0.2, 0.2, 0.4]},\n        ]\n        \n        for i, obj_data in enumerate(objects):\n            create_primitive(\n                prim_path=f"/World/Object{i}",\n                primitive_type=obj_data["type"],\n                position=np.array(obj_data["pos"]),\n                orientation=np.array([0, 0, 0, 1]),\n                scale=np.array(obj_data["size"])\n            )\n    \n    def setup_lighting(self):\n        """Setup realistic lighting for the scene"""\n        # Add a dome light to simulate ambient lighting\n        dome_light = create_primitive(\n            prim_path="/World/DomeLight",\n            primitive_type="DomeLight",\n            position=np.array([0, 0, 0]),\n            scale=np.array([1, 1, 1])\n        )\n        \n        dome_light.GetAttribute("inputs:color").Set(Gf.Vec3f(0.8, 0.8, 0.8))\n        dome_light.GetAttribute("inputs:intensity").Set(3000)\n        \n        # Add a key light (main light source)\n        key_light = create_primitive(\n            prim_path="/World/KeyLight",\n            primitive_type="DistantLight",\n            position=np.array([3, -3, 5]),\n            orientation=np.array([0.3, 0.1, 0, 1])\n        )\n        \n        key_light.GetAttribute("inputs:color").Set(Gf.Vec3f(1.0, 0.95, 0.8))\n        key_light.GetAttribute("inputs:intensity").Set(4000)\n\n# To run this in Isaac Sim:\n# env = PhotorealisticEnvironment()\n# env.world.reset()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"material-and-texturing-for-realism",children:"Material and Texturing for Realism"}),"\n",(0,i.jsx)(e.p,{children:"Creating photorealistic scenes requires attention to materials and textures:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Material Definition"}),": Use Physically-Based Rendering (PBR) materials that accurately reflect light"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Texture Mapping"}),": Apply high-resolution textures to surfaces"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Lighting Setup"}),": Configure realistic lighting with proper intensities and colors"]}),"\n"]}),"\n",(0,i.jsx)(e.p,{children:"In Isaac Sim, materials are defined using the MaterialX standard, which ensures physically accurate rendering. For our humanoid robot's environment, we might create materials for:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Metal surfaces (robot parts)"}),"\n",(0,i.jsx)(e.li,{children:"Fabric materials (furniture)"}),"\n",(0,i.jsx)(e.li,{children:"Wood textures (tables, floors)"}),"\n",(0,i.jsx)(e.li,{children:"Plastic objects (household items)"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"sensor-simulation-in-isaac-sim",children:"Sensor Simulation in Isaac Sim"}),"\n",(0,i.jsx)(e.h3,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,i.jsx)(e.p,{children:"Cameras are critical for visual perception in humanoid robots. Isaac Sim provides realistic camera simulation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\nimport numpy as np\n\ndef setup_camera(robot_prim_path, name="rgb_camera", position=[0.1, 0, 0.1], orientation=[0, 0, 0, 1]):\n    """\n    Setup RGB camera on the robot\n    """\n    camera = Camera(\n        prim_path=f"{robot_prim_path}/{name}",\n        name=name,\n        position=position,\n        frequency=30,  # Hz\n        resolution=(640, 480)\n    )\n    \n    # Configure camera properties\n    camera.set_focal_length(24.0)\n    camera.set_horizontal_aperture(20.955)\n    camera.set_vertical_aperture(15.29)\n    \n    # Enable noise modeling\n    camera.add_noise_model("RgbNoiseModel", intensity=0.1)\n    \n    return camera\n'})}),"\n",(0,i.jsx)(e.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,i.jsx)(e.p,{children:"LiDAR sensors provide 3D spatial information crucial for navigation and mapping:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def setup_lidar(robot_prim_path, name="lidar", position=[0.2, 0, 0.5]):\n    """\n    Setup LiDAR sensor on the robot\n    """\n    from omni.isaac.range_sensor import LidarRtx\n    \n    lidar = LidarRtx(\n        prim_path=f"{robot_prim_path}/{name}",\n        name=name,\n        translation=position,\n        orientation=[0, 0, 0, 1],\n        config="Example_Rotary_M16",  # 16 beam LiDAR\n        visible=True\n    )\n    \n    # Configure LiDAR properties\n    lidar.set_max_range(25.0)  # meters\n    lidar.set_lowest_range(0.1)  # meters\n    \n    return lidar\n'})}),"\n",(0,i.jsx)(e.h3,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,i.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) provide orientation and acceleration data:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def setup_imu(robot_prim_path, name="imu", position=[0, 0, 0.5]):\n    """\n    Setup IMU sensor on the robot\n    """\n    from omni.isaac.core.sensors import ImuSensor\n    \n    imu = ImuSensor(\n        prim_path=f"{robot_prim_path}/{name}",\n        name=name,\n        position=position\n    )\n    \n    # Configure IMU properties\n    imu._sensor.add_noise_imu()  # Add realistic noise\n    \n    return imu\n'})}),"\n",(0,i.jsx)(e.h2,{id:"synthetic-data-generation-workflow",children:"Synthetic Data Generation Workflow"}),"\n",(0,i.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(e.p,{children:"Domain randomization is a technique to make perception models more robust by varying environmental conditions:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import random\nfrom pxr import Gf\n\ndef apply_domain_randomization(world_stage):\n    """Apply domain randomization to the scene"""\n    \n    # Randomize lighting\n    dome_light = world_stage.GetPrimAtPath("/World/DomeLight")\n    if dome_light.IsValid():\n        # Random color temperature (between 4000K and 8000K)\n        color_range = random.uniform(0.9, 1.1)\n        dome_light.GetAttribute("inputs:color").Set(\n            Gf.Vec3f(color_range, color_range * random.uniform(0.95, 1.05), random.uniform(0.9, 1.1))\n        )\n        \n        # Random intensity\n        intensity = random.uniform(2000, 5000)\n        dome_light.GetAttribute("inputs:intensity").Set(intensity)\n    \n    # Randomize object positions\n    for i in range(5):  # Randomize 5 objects\n        obj_prim = world_stage.GetPrimAtPath(f"/World/Object{i}")\n        if obj_prim.IsValid():\n            # Add random position offset\n            current_pos = obj_prim.GetAttribute("xformOp:translate").Get()\n            new_pos = [\n                current_pos[0] + random.uniform(-0.2, 0.2),\n                current_pos[1] + random.uniform(-0.2, 0.2), \n                current_pos[2] + random.uniform(-0.1, 0.1)\n            ]\n            obj_prim.GetAttribute("xformOp:translate").Set(new_pos)\n    \n    # Randomize material properties\n    # This would involve modifying material properties of objects\n    # to vary appearance while maintaining physical properties\n'})}),"\n",(0,i.jsx)(e.h3,{id:"data-collection-pipeline",children:"Data Collection Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"Creating a complete data collection pipeline:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport omni\nfrom omni.isaac.core import World\nfrom omni.vision.annotation import AnnotationManager\nimport json\nfrom datetime import datetime\n\nclass SyntheticDataCollector:\n    def __init__(self, world, output_dir="synthetic_data"):\n        self.world = world\n        self.output_dir = output_dir\n        self.annotation_manager = AnnotationManager()\n        self.frame_count = 0\n        \n    def collect_frame_data(self, cameras, lidars, annotations=True):\n        """Collect synchronized data from all sensors"""\n        \n        # Step the physics engine to get new sensor data\n        self.world.step(render=True)\n        \n        frame_data = {\n            "frame_id": self.frame_count,\n            "timestamp": datetime.now().isoformat(),\n            "sensors": {}\n        }\n        \n        # Collect RGB camera data\n        for cam_name, camera in cameras.items():\n            # Get RGB image\n            rgb_data = camera.get_rgb()\n            if rgb_data is not None:\n                # Save RGB image\n                img_path = f"{self.output_dir}/{cam_name}_rgb_{self.frame_count:06d}.png"\n                cv2.imwrite(img_path, cv2.cvtColor(rgb_data, cv2.COLOR_RGB2BGR))\n                frame_data["sensors"][cam_name] = {"rgb_path": img_path}\n        \n        # Collect LiDAR data\n        for lidar_name, lidar in lidars.items():\n            # Get LiDAR point cloud\n            point_cloud = lidar.get_linear_depth_data()\n            if point_cloud is not None:\n                # Save point cloud data\n                pc_path = f"{self.output_dir}/{lidar_name}_points_{self.frame_count:06d}.npy"\n                np.save(pc_path, point_cloud)\n                frame_data["sensors"][f"{lidar_name}_lidar"] = {"pointcloud_path": pc_path}\n        \n        # Collect annotations if requested\n        if annotations:\n            # Get segmentation masks, bounding boxes, etc.\n            annotations_data = self.collect_annotations()\n            frame_data["annotations"] = annotations_data\n        \n        # Save frame metadata\n        metadata_path = f"{self.output_dir}/frame_{self.frame_count:06d}_metadata.json"\n        with open(metadata_path, \'w\') as f:\n            json.dump(frame_data, f, indent=2)\n        \n        self.frame_count += 1\n        \n        return frame_data\n    \n    def collect_annotations(self):\n        """Collect ground truth annotations"""\n        annotations = {}\n        \n        # Example: 2D bounding boxes for objects\n        # This would use Isaac\'s annotation tools to get ground truth\n        bbox_annotations = self.annotation_manager.get_2d_bounding_box_annotations()\n        annotations["bounding_boxes"] = bbox_annotations\n        \n        # Example: Semantic segmentation\n        seg_annotations = self.annotation_manager.get_semantic_segmentation_annotations()\n        annotations["semantic_segmentation"] = seg_annotations\n        \n        return annotations\n'})}),"\n",(0,i.jsx)(e.h2,{id:"generating-training-datasets",children:"Generating Training Datasets"}),"\n",(0,i.jsx)(e.h3,{id:"classification-dataset",children:"Classification Dataset"}),"\n",(0,i.jsx)(e.p,{children:"For object classification, generate a dataset with labeled images:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\ndef generate_classification_dataset(synthetic_data_path, output_path, class_names):\n    """\n    Organize synthetic data into classification dataset format\n    """\n    # Create train/validation/test splits\n    all_images = []\n    all_labels = []\n    \n    # Collect all synthetic images and their labels\n    for class_name in class_names:\n        class_images = [f for f in os.listdir(synthetic_data_path) \n                       if class_name in f and f.endswith(\'.png\')]\n        \n        for img in class_images:\n            all_images.append(img)\n            all_labels.append(class_name)\n    \n    # Split the data\n    train_imgs, temp_imgs, train_labels, temp_labels = train_test_split(\n        all_images, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n    )\n    \n    val_imgs, test_imgs, val_labels, test_labels = train_test_split(\n        temp_imgs, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n    )\n    \n    # Create directory structure\n    for split_name, split_data in [("train", (train_imgs, train_labels)), \n                                   ("val", (val_imgs, val_labels)), \n                                   ("test", (test_imgs, test_labels))]:\n        split_path = os.path.join(output_path, split_name)\n        \n        for class_name in class_names:\n            class_path = os.path.join(split_path, class_name)\n            os.makedirs(class_path, exist_ok=True)\n        \n        # Move images to appropriate directories\n        images, labels = split_data\n        for img, label in zip(images, labels):\n            src_path = os.path.join(synthetic_data_path, img)\n            dst_path = os.path.join(split_path, label, img)\n            shutil.copy2(src_path, dst_path)\n'})}),"\n",(0,i.jsx)(e.h3,{id:"detection-dataset",children:"Detection Dataset"}),"\n",(0,i.jsx)(e.p,{children:"For object detection, format data in standard formats like COCO:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def generate_detection_dataset(synthetic_data_path, output_path):\n    """\n    Generate detection dataset in COCO format\n    """\n    import json\n    \n    coco_format = {\n        "info": {\n            "year": 2024,\n            "version": "1.0",\n            "description": "Synthetic Humanoid Robot Perception Dataset",\n            "contributor": "Physical AI & Humanoid Robotics Project",\n            "url": "",\n            "date_created": datetime.now().isoformat()\n        },\n        "licenses": [],\n        "images": [],\n        "annotations": [],\n        "categories": []\n    }\n    \n    # Add categories (object classes)\n    class_names = ["cube", "sphere", "cylinder", "humanoid_robot", "table"]\n    for i, name in enumerate(class_names):\n        coco_format["categories"].append({\n            "id": i,\n            "name": name,\n            "supercategory": "object"\n        })\n    \n    # Process each synthetic image and its annotations\n    metadata_files = [f for f in os.listdir(synthetic_data_path) \n                     if f.endswith(\'_metadata.json\')]\n    \n    image_id = 0\n    annotation_id = 0\n    \n    for meta_file in metadata_files:\n        meta_path = os.path.join(synthetic_data_path, meta_file)\n        with open(meta_path, \'r\') as f:\n            frame_data = json.load(f)\n        \n        # Add image entry\n        img_path = next(iter(frame_data["sensors"].values()))["rgb_path"]\n        img_filename = os.path.basename(img_path)\n        \n        coco_format["images"].append({\n            "id": image_id,\n            "license": 0,\n            "file_name": img_filename,\n            "height": 480,\n            "width": 640,\n            "date_captured": frame_data["timestamp"]\n        })\n        \n        # Add annotations (bounding boxes)\n        if "annotations" in frame_data and "bounding_boxes" in frame_data["annotations"]:\n            for bbox in frame_data["annotations"]["bounding_boxes"]:\n                # Assuming bbox format is [x, y, width, height]\n                coco_format["annotations"].append({\n                    "id": annotation_id,\n                    "image_id": image_id,\n                    "category_id": bbox["category_id"],\n                    "bbox": bbox["bbox"],  # [x, y, width, height]\n                    "area": bbox["bbox"][2] * bbox["bbox"][3],\n                    "iscrowd": 0\n                })\n                annotation_id += 1\n        \n        image_id += 1\n    \n    # Save the COCO format dataset\n    with open(os.path.join(output_path, "annotations.json"), \'w\') as f:\n        json.dump(coco_format, f, indent=2)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"validation-of-synthetic-data-quality",children:"Validation of Synthetic Data Quality"}),"\n",(0,i.jsx)(e.h3,{id:"comparing-with-real-data",children:"Comparing with Real Data"}),"\n",(0,i.jsx)(e.p,{children:"To validate synthetic data quality, compare it with real data:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Statistical Similarity"}),": Compare distributions of image features"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Model Performance"}),": Train models on synthetic and real data, compare performance"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Domain Gap Metrics"}),": Use metrics like Fr\xe9chet Inception Distance (FID)"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"quality-assessment-techniques",children:"Quality Assessment Techniques"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def assess_synthetic_data_quality(synthetic_images, real_images):\n    """\n    Assess the quality of synthetic data vs real data\n    """\n    import torch\n    import torchvision.transforms as transforms\n    from torch_fidelity import calculate_metrics\n    \n    # Calculate FID (Fr\xe9chet Inception Distance)\n    # Lower FID indicates better similarity between datasets\n    metrics = calculate_metrics(\n        synthetic_images, \n        real_images,\n        cuda=True,\n        isc=True,  # Inception Score\n        fid=True,  # Fr\xe9chet Inception Distance\n        verbose=False\n    )\n    \n    print(f"Synthetic Data Quality Metrics:")\n    print(f"  FID Score: {metrics[\'frechet_inception_distance\']:.2f}")\n    print(f"  Inception Score: {metrics[\'inception_score_mean\']:.2f}")\n    \n    return metrics\n'})}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-21-create-a-perception-scene",children:"Hands-on Exercise 2.1: Create a Perception Scene"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Create a new Isaac Sim scene with:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"A humanoid robot model"}),"\n",(0,i.jsx)(e.li,{children:"Various household objects for perception"}),"\n",(0,i.jsx)(e.li,{children:"Proper lighting setup"}),"\n",(0,i.jsx)(e.li,{children:"At least one RGB camera and one LiDAR sensor"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Implement domain randomization by varying:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Object positions and orientations"}),"\n",(0,i.jsx)(e.li,{children:"Lighting conditions"}),"\n",(0,i.jsx)(e.li,{children:"Background textures"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Set up a basic data collection pipeline that captures:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"RGB images from the camera"}),"\n",(0,i.jsx)(e.li,{children:"Point cloud data from the LiDAR"}),"\n",(0,i.jsx)(e.li,{children:"Basic annotations (object poses)"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsx)(e.p,{children:"Generate at least 50 synthetic frames with annotations"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"hands-on-exercise-22-generate-a-perception-dataset",children:"Hands-on Exercise 2.2: Generate a Perception Dataset"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Organize your collected synthetic data into a proper dataset format"}),"\n",(0,i.jsx)(e.li,{children:"Create a classification dataset with at least 3 object classes"}),"\n",(0,i.jsx)(e.li,{children:"Generate COCO-style annotations for object detection"}),"\n",(0,i.jsx)(e.li,{children:"Validate the dataset format using a basic dataset loader"}),"\n",(0,i.jsx)(e.li,{children:"Create a visualization script to verify annotations are correct"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"validation-checklist",children:"Validation Checklist"}),"\n",(0,i.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I understand the importance of perception in humanoid robotics"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can create photorealistic scenes in Isaac Sim with USD"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I know how to set up and configure different sensor types (camera, LiDAR, IMU)"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can implement domain randomization techniques for synthetic data"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have created a complete data collection pipeline"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I can format synthetic data into standard dataset formats (classification, detection)"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have validated the quality of my synthetic data"]}),"\n",(0,i.jsxs)(e.li,{className:"task-list-item",children:[(0,i.jsx)(e.input,{type:"checkbox",disabled:!0})," ","I have generated a usable perception dataset for training models"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter covered the creation of perception systems for humanoid robots using Isaac Sim's photorealistic simulation capabilities. We explored scene composition with USD, sensor simulation, and synthetic data generation workflows. We also covered domain randomization techniques and dataset formatting for machine learning applications."}),"\n",(0,i.jsx)(e.p,{children:"The combination of Isaac Sim's realistic rendering and physically-accurate simulation makes it ideal for generating training data that can bridge the sim-to-real gap for humanoid robot perception systems. The synthetic data generation pipeline we've explored enables rapid development and testing of perception models before deployment on physical robots."}),"\n",(0,i.jsx)(e.p,{children:"In the next chapter, we'll explore navigation and path planning for humanoid robots using Isaac ROS and Nav2."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const a={},o=i.createContext(a);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);